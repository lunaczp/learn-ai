{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunaczp/learn-ai/blob/main/notebooks/llama-light/convert_and_quantiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 转换并量化中文LLaMA和Alpaca模型\n",
        "\n",
        "项目地址：https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
        "\n",
        "⚠️ 内存消耗提示（确保刷出来的机器RAM大于以下要求）：\n",
        "- 7B模型：15G+\n",
        "- 13B模型：18G+\n",
        "- 33B模型：22G+\n",
        "\n",
        "💡 提示和小窍门：\n",
        "- 免费用户默认的内存只有12G左右，不足以转换模型。**实测选择TPU的话有机会随机出35G内存**，建议多试几次\n",
        "- Pro(+)用户请选择 “代码执行程序” -> “更改运行时类型” -> “高RAM”\n",
        "- 程序莫名崩掉或断开连接就说明内存爆了\n",
        "- 如果选了“高RAM”之后内存还是不够大的话，选择以下操作，有的时候会分配出很高内存的机器，祝你好运😄！\n",
        "    - 可以把GPU或者TPU也选上（虽然不会用到）\n",
        "    - 选GPU时，Pro(+)用户可选“A100”类型GPU\n",
        "\n",
        "*温馨提示：用完之后注意断开运行时，选择满足要求的最低配置即可，避免不必要的计算单元消耗（Pro只给100个计算单元）。*"
      ],
      "metadata": {
        "id": "B1c96_k3MahN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安装相关依赖"
      ],
      "metadata": {
        "id": "vScqHD_jMFOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5WKFJXIL6ZU",
        "outputId": "d0adca50-6dee-4c2f-a5ca-bae6c417d7a4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 20 10:53:57 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   50C    P8              13W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.11.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.43.0)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n",
            "Collecting transformers==4.30.2\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2024.2.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.2\n",
            "    Uninstalling transformers-4.40.2:\n",
            "      Successfully uninstalled transformers-4.40.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting peft==0.3.0\n",
            "  Downloading peft-0.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (1.13.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (4.30.2)\n",
            "Collecting accelerate (from peft==0.3.0)\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (4.11.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0) (0.43.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate->peft==0.3.0) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->peft==0.3.0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (3.14.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate->peft==0.3.0) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0) (2024.2.2)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-0.30.1 peft-0.3.0\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!pip install torch==1.13.1\n",
        "!pip install transformers==4.30.2\n",
        "!pip install peft==0.3.0\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 克隆目录和代码"
      ],
      "metadata": {
        "id": "ygb1xFIMNQKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCEJh7NJNXz9",
        "outputId": "c5433f4c-648c-40db-8c5a-fccf34c216ca",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chinese-LLaMA-Alpaca'...\n",
            "remote: Enumerating objects: 2178, done.\u001b[K\n",
            "remote: Counting objects: 100% (520/520), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 2178 (delta 444), reused 409 (delta 402), pack-reused 1658\u001b[K\n",
            "Receiving objects: 100% (2178/2178), 23.51 MiB | 22.23 MiB/s, done.\n",
            "Resolving deltas: 100% (1356/1356), done.\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 25003, done.\u001b[K\n",
            "remote: Counting objects: 100% (9011/9011), done.\u001b[K\n",
            "remote: Compressing objects: 100% (518/518), done.\u001b[K\n",
            "remote: Total 25003 (delta 8780), reused 8521 (delta 8492), pack-reused 15992\u001b[K\n",
            "Receiving objects: 100% (25003/25003), 44.43 MiB | 19.33 MiB/s, done.\n",
            "Resolving deltas: 100% (17802/17802), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并模型（以Alpaca-7B为例）\n",
        "\n",
        "此处使用的是🤗模型库中提供的基模型（已是HF格式），而不是Facebook官方的LLaMA模型，因此略去将原版LLaMA转换为HF格式的步骤。\n",
        "**这里直接运行第二步：合并LoRA权重**，生成全量模型权重。可以直接指定🤗模型库的地址，也可以是本地存放地址。\n",
        "- 基模型：`elinas/llama-7b-hf-transformers-4.29` *（use at your own risk，我们比对过SHA256和正版一致，但你应确保自己有权使用该模型）*\n",
        "- LoRA模型：`ziqingyang/chinese-alpaca-lora-7b`\n",
        "   - 如果是Alpaca-Plus模型，记得要同时传入llama和alpaca的lora，教程：[这里](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/手动模型合并与转换#多lora权重合并适用于chinese-alpaca-plus)\n",
        "- 输出格式：可选pth或者huggingface，这里选择pth，因为后面要用llama.cpp量化\n",
        "\n",
        "由于要下载模型，所以需要耐心等待一下，尤其是33B模型。\n",
        "转换好的模型存放在`alpaca-combined`目录。\n",
        "如果你不需要量化模型，那么到这一步就结束了，可自行下载或者转存到Google Drive。"
      ],
      "metadata": {
        "id": "nIyxX0DSNsgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./Chinese-LLaMA-Alpaca/scripts/merge_llama_with_chinese_lora_low_mem.py \\\n",
        "    --base_model 'elinas/llama-7b-hf-transformers-4.29' \\\n",
        "    --lora_model 'hfl/chinese-alpaca-lora-7b' \\\n",
        "    --output_type pth \\\n",
        "    --output_dir alpaca-combined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AV4EW5hNhVV",
        "outputId": "c66e514a-3855-409d-af21-6d6ed6441382",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model: elinas/llama-7b-hf-transformers-4.29\n",
            "LoRA model(s) ['hfl/chinese-alpaca-lora-7b']:\n",
            "Loading hfl/chinese-alpaca-lora-7b\n",
            "Cannot find lora model on the disk. Downloading lora model from hub...\n",
            "Fetching 7 files:   0% 0/7 [00:00<?, ?it/s]\n",
            ".gitattributes: 100% 1.48k/1.48k [00:00<00:00, 9.68MB/s]\n",
            "Fetching 7 files:  14% 1/7 [00:00<00:02,  2.06it/s]\n",
            "adapter_config.json: 100% 472/472 [00:00<00:00, 3.61MB/s]\n",
            "\n",
            "tokenizer_config.json: 100% 166/166 [00:00<00:00, 1.22MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 96.0/96.0 [00:00<00:00, 621kB/s]\n",
            "\n",
            "README.md: 100% 316/316 [00:00<00:00, 2.25MB/s]\n",
            "Fetching 7 files:  29% 2/7 [00:00<00:01,  3.01it/s]\n",
            "adapter_model.bin:   0% 0.00/858M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "tokenizer.model:   0% 0.00/758k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "adapter_model.bin:   1% 10.5M/858M [00:00<00:18, 46.1MB/s]\u001b[A\n",
            "\n",
            "tokenizer.model: 100% 758k/758k [00:00<00:00, 2.47MB/s]\n",
            "\n",
            "adapter_model.bin:   2% 21.0M/858M [00:00<00:16, 52.3MB/s]\u001b[A\n",
            "adapter_model.bin:   4% 31.5M/858M [00:00<00:12, 63.7MB/s]\u001b[A\n",
            "adapter_model.bin:   5% 41.9M/858M [00:00<00:11, 69.2MB/s]\u001b[A\n",
            "adapter_model.bin:   6% 52.4M/858M [00:00<00:10, 77.0MB/s]\u001b[A\n",
            "adapter_model.bin:   9% 73.4M/858M [00:00<00:08, 97.8MB/s]\u001b[A\n",
            "adapter_model.bin:  11% 94.4M/858M [00:01<00:06, 113MB/s] \u001b[A\n",
            "adapter_model.bin:  13% 115M/858M [00:01<00:06, 122MB/s] \u001b[A\n",
            "adapter_model.bin:  16% 136M/858M [00:01<00:05, 129MB/s]\u001b[A\n",
            "adapter_model.bin:  18% 157M/858M [00:01<00:05, 132MB/s]\u001b[A\n",
            "adapter_model.bin:  21% 178M/858M [00:01<00:05, 132MB/s]\u001b[A\n",
            "adapter_model.bin:  23% 199M/858M [00:01<00:04, 134MB/s]\u001b[A\n",
            "adapter_model.bin:  26% 220M/858M [00:01<00:04, 136MB/s]\u001b[A\n",
            "adapter_model.bin:  28% 241M/858M [00:02<00:04, 138MB/s]\u001b[A\n",
            "adapter_model.bin:  31% 262M/858M [00:02<00:04, 139MB/s]\u001b[A\n",
            "adapter_model.bin:  33% 283M/858M [00:02<00:04, 140MB/s]\u001b[A\n",
            "adapter_model.bin:  35% 304M/858M [00:02<00:03, 141MB/s]\u001b[A\n",
            "adapter_model.bin:  38% 325M/858M [00:02<00:03, 141MB/s]\u001b[A\n",
            "adapter_model.bin:  40% 346M/858M [00:02<00:03, 142MB/s]\u001b[A\n",
            "adapter_model.bin:  43% 367M/858M [00:03<00:03, 134MB/s]\u001b[A\n",
            "adapter_model.bin:  45% 388M/858M [00:03<00:03, 122MB/s]\u001b[A\n",
            "adapter_model.bin:  48% 409M/858M [00:03<00:03, 117MB/s]\u001b[A\n",
            "adapter_model.bin:  50% 430M/858M [00:03<00:03, 114MB/s]\u001b[A\n",
            "adapter_model.bin:  53% 451M/858M [00:03<00:03, 112MB/s]\u001b[A\n",
            "adapter_model.bin:  55% 472M/858M [00:04<00:03, 113MB/s]\u001b[A\n",
            "adapter_model.bin:  57% 493M/858M [00:04<00:03, 114MB/s]\u001b[A\n",
            "adapter_model.bin:  60% 514M/858M [00:04<00:02, 115MB/s]\u001b[A\n",
            "adapter_model.bin:  62% 535M/858M [00:04<00:02, 117MB/s]\u001b[A\n",
            "adapter_model.bin:  65% 556M/858M [00:04<00:02, 119MB/s]\u001b[A\n",
            "adapter_model.bin:  67% 577M/858M [00:04<00:02, 118MB/s]\u001b[A\n",
            "adapter_model.bin:  70% 598M/858M [00:05<00:02, 122MB/s]\u001b[A\n",
            "adapter_model.bin:  72% 619M/858M [00:05<00:01, 126MB/s]\u001b[A\n",
            "adapter_model.bin:  75% 640M/858M [00:05<00:01, 129MB/s]\u001b[A\n",
            "adapter_model.bin:  77% 661M/858M [00:05<00:01, 131MB/s]\u001b[A\n",
            "adapter_model.bin:  79% 682M/858M [00:05<00:01, 134MB/s]\u001b[A\n",
            "adapter_model.bin:  82% 703M/858M [00:05<00:01, 137MB/s]\u001b[A\n",
            "adapter_model.bin:  84% 724M/858M [00:05<00:00, 138MB/s]\u001b[A\n",
            "adapter_model.bin:  87% 744M/858M [00:06<00:00, 139MB/s]\u001b[A\n",
            "adapter_model.bin:  89% 765M/858M [00:06<00:00, 139MB/s]\u001b[A\n",
            "adapter_model.bin:  92% 786M/858M [00:06<00:00, 139MB/s]\u001b[A\n",
            "adapter_model.bin:  94% 807M/858M [00:06<00:00, 140MB/s]\u001b[A\n",
            "adapter_model.bin:  96% 828M/858M [00:06<00:00, 140MB/s]\u001b[A\n",
            "adapter_model.bin: 100% 858M/858M [00:06<00:00, 124MB/s]\n",
            "Fetching 7 files: 100% 7/7 [00:07<00:00,  1.11s/it]\n",
            "Cannot find lora model on the disk. Downloading lora model from hub...\n",
            "Fetching 11 files:   0% 0/11 [00:00<?, ?it/s]\n",
            "config.json: 100% 507/507 [00:00<00:00, 3.46MB/s]\n",
            "\n",
            "README.md: 100% 8.34k/8.34k [00:00<00:00, 27.3MB/s]\n",
            "\n",
            "pytorch_model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 116MB/s]\n",
            "\n",
            ".gitattributes: 100% 1.48k/1.48k [00:00<00:00, 11.9MB/s]\n",
            "Fetching 11 files:   9% 1/11 [00:00<00:01,  5.32it/s]\n",
            "generation_config.json: 100% 137/137 [00:00<00:00, 976kB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 411/411 [00:00<00:00, 2.52MB/s]\n",
            "\n",
            "tokenizer_config.json: 100% 727/727 [00:00<00:00, 4.74MB/s]\n",
            "\n",
            "tokenizer.json:   0% 0.00/1.84M [00:00<?, ?B/s]\u001b[A\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 9.30MB/s]\n",
            "\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.model:   0% 0.00/500k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 1.92MB/s]\n",
            "\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 10.5M/9.98G [00:00<08:47, 18.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   0% 10.5M/3.50G [00:00<03:27, 16.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 21.0M/9.98G [00:00<05:10, 32.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 21.0M/3.50G [00:00<02:02, 28.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 31.5M/9.98G [00:00<03:44, 44.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 31.5M/3.50G [00:00<01:29, 38.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   0% 41.9M/9.98G [00:00<03:02, 54.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 41.9M/3.50G [00:01<01:12, 47.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 52.4M/9.98G [00:01<02:48, 59.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 52.4M/3.50G [00:01<01:09, 49.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 62.9M/9.98G [00:01<02:46, 59.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 62.9M/3.50G [00:01<01:01, 56.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 73.4M/9.98G [00:01<02:32, 64.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 73.4M/3.50G [00:01<01:01, 55.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 83.9M/9.98G [00:01<02:34, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 94.4M/9.98G [00:01<02:39, 62.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 83.9M/3.50G [00:01<01:02, 54.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 105M/9.98G [00:01<02:28, 66.7MB/s] \u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 94.4M/3.50G [00:02<01:02, 54.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 115M/9.98G [00:02<02:32, 64.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 105M/3.50G [00:02<00:56, 59.7MB/s] \u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 126M/9.98G [00:02<02:22, 69.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 115M/3.50G [00:02<00:58, 57.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 136M/9.98G [00:02<02:33, 63.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 126M/3.50G [00:02<00:54, 61.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   1% 147M/9.98G [00:02<02:31, 64.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 136M/3.50G [00:02<00:58, 57.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 157M/9.98G [00:02<02:28, 66.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 147M/3.50G [00:02<00:57, 58.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 168M/9.98G [00:02<02:34, 63.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 157M/3.50G [00:03<00:55, 60.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 178M/9.98G [00:03<02:34, 63.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 168M/3.50G [00:03<00:55, 60.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 189M/9.98G [00:03<02:26, 66.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 178M/3.50G [00:03<00:53, 61.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 199M/9.98G [00:03<02:32, 64.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 210M/9.98G [00:03<02:21, 68.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 189M/3.50G [00:03<00:56, 59.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 220M/9.98G [00:03<02:27, 65.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 199M/3.50G [00:03<00:55, 59.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 231M/9.98G [00:03<02:32, 63.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 210M/3.50G [00:03<00:53, 61.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   2% 241M/9.98G [00:03<02:22, 68.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 220M/3.50G [00:04<00:53, 60.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 252M/9.98G [00:04<02:28, 65.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 231M/3.50G [00:04<00:52, 62.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 262M/9.98G [00:04<02:32, 63.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 241M/3.50G [00:04<00:54, 59.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 273M/9.98G [00:04<02:22, 67.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 252M/3.50G [00:04<00:54, 59.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 283M/9.98G [00:04<02:27, 65.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 262M/3.50G [00:04<00:52, 61.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 294M/9.98G [00:04<02:31, 63.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 273M/3.50G [00:04<00:53, 60.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 304M/9.98G [00:05<02:51, 56.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 283M/3.50G [00:05<00:51, 62.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 315M/9.98G [00:05<02:34, 62.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 294M/3.50G [00:05<00:53, 59.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 325M/9.98G [00:05<02:38, 61.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 304M/3.50G [00:05<00:53, 59.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 336M/9.98G [00:05<02:26, 66.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 315M/3.50G [00:05<00:51, 61.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   3% 346M/9.98G [00:05<02:29, 64.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 325M/3.50G [00:05<00:52, 60.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 357M/9.98G [00:05<02:34, 62.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 367M/9.98G [00:05<02:22, 67.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 336M/3.50G [00:06<00:54, 58.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 346M/3.50G [00:06<00:52, 60.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 377M/9.98G [00:06<02:28, 64.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 388M/9.98G [00:06<02:19, 68.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 357M/3.50G [00:06<00:52, 60.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 398M/9.98G [00:06<02:26, 65.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 367M/3.50G [00:06<00:54, 57.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 409M/9.98G [00:06<02:31, 63.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 377M/3.50G [00:06<00:51, 60.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 419M/9.98G [00:06<02:34, 62.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 388M/3.50G [00:06<00:51, 60.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 430M/9.98G [00:06<02:23, 66.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 398M/3.50G [00:07<00:53, 58.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 409M/3.50G [00:07<00:51, 60.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 440M/9.98G [00:07<02:50, 55.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 419M/3.50G [00:07<00:51, 59.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 451M/9.98G [00:07<02:49, 56.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 461M/9.98G [00:07<02:33, 62.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 430M/3.50G [00:07<00:49, 62.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 472M/9.98G [00:07<02:34, 61.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 440M/3.50G [00:07<00:51, 59.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 482M/9.98G [00:07<02:21, 66.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 451M/3.50G [00:07<00:51, 58.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 493M/9.98G [00:07<02:28, 64.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 461M/3.50G [00:08<00:49, 61.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 503M/9.98G [00:08<02:31, 62.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 472M/3.50G [00:08<00:50, 60.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 514M/9.98G [00:08<02:19, 67.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 482M/3.50G [00:08<00:52, 58.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 524M/9.98G [00:08<02:26, 64.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 493M/3.50G [00:08<00:49, 60.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 535M/9.98G [00:08<02:30, 62.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 503M/3.50G [00:08<00:50, 59.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   5% 545M/9.98G [00:08<02:18, 67.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 514M/3.50G [00:08<00:48, 62.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 556M/9.98G [00:08<02:24, 65.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 524M/3.50G [00:09<00:50, 59.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 566M/9.98G [00:09<02:29, 63.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 577M/9.98G [00:09<02:19, 67.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 535M/3.50G [00:09<00:50, 59.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 587M/9.98G [00:09<02:23, 65.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 545M/3.50G [00:09<00:48, 61.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 598M/9.98G [00:09<02:14, 69.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 556M/3.50G [00:09<00:48, 60.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 608M/9.98G [00:09<02:22, 66.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 566M/3.50G [00:09<00:47, 62.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 619M/9.98G [00:09<02:26, 64.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 577M/3.50G [00:10<00:49, 59.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 629M/9.98G [00:10<02:16, 68.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 587M/3.50G [00:10<00:48, 59.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   6% 640M/9.98G [00:10<02:22, 65.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 598M/3.50G [00:10<00:47, 61.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 650M/9.98G [00:10<02:26, 63.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 608M/3.50G [00:10<00:47, 60.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 661M/9.98G [00:10<02:19, 66.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 619M/3.50G [00:10<00:46, 62.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 671M/9.98G [00:10<02:21, 65.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 682M/9.98G [00:10<02:12, 70.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 629M/3.50G [00:10<00:48, 59.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 692M/9.98G [00:10<02:20, 66.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 640M/3.50G [00:11<00:48, 59.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 703M/9.98G [00:11<02:26, 63.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 650M/3.50G [00:11<00:46, 61.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 661M/3.50G [00:11<00:46, 60.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 713M/9.98G [00:11<02:34, 59.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 671M/3.50G [00:11<00:45, 62.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 724M/9.98G [00:11<02:50, 54.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 682M/3.50G [00:11<00:47, 59.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 734M/9.98G [00:11<02:44, 56.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 692M/3.50G [00:11<00:47, 59.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 744M/9.98G [00:11<02:41, 57.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 703M/3.50G [00:12<00:45, 61.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 755M/9.98G [00:12<02:25, 63.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 713M/3.50G [00:12<00:47, 58.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 765M/9.98G [00:12<02:29, 61.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 776M/9.98G [00:12<02:17, 66.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 724M/3.50G [00:12<00:47, 58.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 786M/9.98G [00:12<02:24, 63.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 734M/3.50G [00:12<00:45, 60.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 797M/9.98G [00:12<02:26, 62.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 744M/3.50G [00:12<00:45, 60.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 807M/9.98G [00:12<02:15, 67.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 755M/3.50G [00:12<00:47, 58.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 818M/9.98G [00:13<02:20, 65.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 765M/3.50G [00:13<00:45, 59.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 828M/9.98G [00:13<02:24, 63.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   8% 839M/9.98G [00:13<02:15, 67.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 776M/3.50G [00:13<00:54, 50.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 849M/9.98G [00:13<02:19, 65.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 786M/3.50G [00:13<00:56, 48.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 860M/9.98G [00:13<02:23, 63.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 870M/9.98G [00:13<02:14, 67.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 807M/3.50G [00:13<00:47, 56.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 881M/9.98G [00:14<02:19, 65.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 818M/3.50G [00:14<00:44, 60.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 891M/9.98G [00:14<02:11, 68.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 828M/3.50G [00:14<00:45, 58.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 902M/9.98G [00:14<02:17, 66.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 839M/3.50G [00:14<00:46, 57.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 912M/9.98G [00:14<02:21, 64.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 849M/3.50G [00:14<00:43, 61.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 923M/9.98G [00:14<02:12, 68.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 860M/3.50G [00:14<00:44, 59.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 933M/9.98G [00:14<02:17, 65.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 870M/3.50G [00:14<00:41, 62.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   9% 944M/9.98G [00:14<02:21, 64.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 954M/9.98G [00:15<02:12, 68.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 881M/3.50G [00:15<00:43, 59.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 965M/9.98G [00:15<02:17, 65.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 891M/3.50G [00:15<00:45, 57.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 975M/9.98G [00:15<02:10, 69.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 902M/3.50G [00:15<00:41, 62.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 986M/9.98G [00:15<02:15, 66.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 912M/3.50G [00:15<00:43, 59.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 996M/9.98G [00:15<02:19, 64.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 923M/3.50G [00:15<00:44, 57.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.01G/9.98G [00:15<02:10, 68.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 933M/3.50G [00:16<00:41, 62.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.02G/9.98G [00:16<02:16, 65.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 944M/3.50G [00:16<00:42, 59.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.03G/9.98G [00:16<02:19, 64.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.04G/9.98G [00:16<02:11, 68.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 954M/3.50G [00:16<00:43, 58.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.05G/9.98G [00:16<02:15, 65.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 965M/3.50G [00:16<00:44, 57.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.06G/9.98G [00:16<02:08, 69.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 975M/3.50G [00:16<00:43, 58.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.07G/9.98G [00:16<02:13, 66.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 986M/3.50G [00:16<00:44, 56.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.08G/9.98G [00:17<02:17, 64.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 996M/3.50G [00:17<00:45, 55.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.09G/9.98G [00:17<02:09, 68.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.01G/3.50G [00:17<00:41, 60.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.10G/9.98G [00:17<02:14, 65.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.11G/9.98G [00:17<02:07, 69.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.02G/3.50G [00:17<00:42, 58.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.12G/9.98G [00:17<02:13, 66.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.03G/3.50G [00:17<00:43, 56.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.13G/9.98G [00:17<02:17, 64.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.04G/3.50G [00:17<00:40, 61.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.14G/9.98G [00:17<02:09, 68.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.05G/3.50G [00:18<00:48, 50.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.15G/9.98G [00:18<02:14, 65.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.16G/9.98G [00:18<02:17, 64.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.06G/3.50G [00:18<00:50, 48.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.17G/9.98G [00:18<02:09, 68.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.07G/3.50G [00:18<00:48, 49.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.18G/9.98G [00:18<02:13, 65.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.08G/3.50G [00:18<00:44, 55.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.20G/9.98G [00:18<02:06, 69.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.09G/3.50G [00:18<00:43, 55.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.21G/9.98G [00:18<02:11, 66.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.10G/3.50G [00:19<00:40, 59.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.22G/9.98G [00:19<02:15, 64.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.11G/3.50G [00:19<00:41, 57.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.23G/9.98G [00:19<02:18, 63.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.24G/9.98G [00:19<02:10, 67.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.12G/3.50G [00:19<00:41, 56.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.25G/9.98G [00:19<02:14, 65.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.13G/3.50G [00:19<00:39, 60.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.26G/9.98G [00:19<02:07, 68.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.14G/3.50G [00:19<00:40, 58.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.27G/9.98G [00:19<02:11, 66.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.15G/3.50G [00:19<00:37, 62.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.28G/9.98G [00:20<02:14, 64.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.16G/3.50G [00:20<00:39, 59.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.29G/9.98G [00:20<02:06, 68.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.17G/3.50G [00:20<00:40, 57.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.30G/9.98G [00:20<02:11, 66.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.18G/3.50G [00:20<00:37, 61.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.31G/9.98G [00:20<02:04, 69.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.20G/3.50G [00:20<00:38, 59.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.32G/9.98G [00:20<02:10, 66.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.21G/3.50G [00:20<00:39, 57.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.33G/9.98G [00:20<02:15, 64.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.34G/9.98G [00:20<02:05, 68.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.22G/3.50G [00:20<00:37, 61.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.35G/9.98G [00:21<02:11, 65.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.23G/3.50G [00:21<00:38, 58.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.36G/9.98G [00:21<02:15, 63.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.24G/3.50G [00:21<00:36, 62.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.37G/9.98G [00:21<02:07, 67.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.25G/3.50G [00:21<00:37, 59.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.38G/9.98G [00:21<02:11, 65.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.26G/3.50G [00:21<00:38, 57.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.39G/9.98G [00:21<02:14, 63.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.27G/3.50G [00:21<00:36, 61.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.41G/9.98G [00:21<02:06, 67.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.28G/3.50G [00:22<00:37, 58.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.42G/9.98G [00:22<02:11, 65.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.29G/3.50G [00:22<00:38, 57.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.43G/9.98G [00:22<02:14, 63.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.30G/3.50G [00:22<00:35, 61.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.44G/9.98G [00:22<02:05, 67.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.31G/3.50G [00:22<00:36, 59.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.45G/9.98G [00:22<02:09, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.46G/9.98G [00:22<02:02, 69.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.32G/3.50G [00:22<00:37, 57.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.47G/9.98G [00:22<02:08, 66.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.33G/3.50G [00:22<00:35, 61.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.48G/9.98G [00:23<02:13, 63.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.34G/3.50G [00:23<00:36, 59.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.49G/9.98G [00:23<02:04, 68.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.35G/3.50G [00:23<00:34, 63.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.50G/9.98G [00:23<02:09, 65.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.36G/3.50G [00:23<00:35, 59.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.51G/9.98G [00:23<02:12, 63.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.37G/3.50G [00:23<00:36, 58.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.52G/9.98G [00:23<02:04, 68.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.38G/3.50G [00:23<00:34, 62.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.53G/9.98G [00:23<02:08, 65.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.39G/3.50G [00:23<00:35, 59.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.54G/9.98G [00:23<02:12, 63.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.41G/3.50G [00:24<00:33, 63.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.55G/9.98G [00:24<02:04, 67.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.56G/9.98G [00:24<02:10, 64.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.42G/3.50G [00:24<00:38, 54.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.57G/9.98G [00:24<02:13, 62.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.43G/3.50G [00:24<00:35, 58.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.58G/9.98G [00:24<02:14, 62.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.44G/3.50G [00:24<00:36, 57.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.59G/9.98G [00:24<02:05, 66.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.45G/3.50G [00:24<00:36, 56.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.60G/9.98G [00:24<02:09, 64.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.46G/3.50G [00:25<00:33, 60.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.61G/9.98G [00:25<02:02, 68.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.47G/3.50G [00:25<00:34, 58.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.63G/9.98G [00:25<02:07, 65.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.48G/3.50G [00:25<00:32, 62.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.64G/9.98G [00:25<02:09, 64.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.49G/3.50G [00:25<00:33, 59.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.65G/9.98G [00:25<02:02, 67.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.66G/9.98G [00:25<02:06, 65.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.50G/3.50G [00:25<00:34, 57.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.67G/9.98G [00:25<02:10, 63.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.51G/3.50G [00:25<00:35, 56.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.68G/9.98G [00:26<02:03, 67.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.52G/3.50G [00:26<00:33, 60.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.69G/9.98G [00:26<02:06, 65.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.53G/3.50G [00:26<00:33, 58.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.70G/9.98G [00:26<02:08, 64.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.54G/3.50G [00:26<00:34, 56.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.71G/9.98G [00:26<02:05, 66.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.55G/3.50G [00:26<00:32, 60.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.72G/9.98G [00:26<02:06, 65.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.56G/3.50G [00:26<00:33, 58.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.73G/9.98G [00:26<02:02, 67.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.57G/3.50G [00:27<00:33, 56.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.74G/9.98G [00:27<02:04, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.75G/9.98G [00:27<02:05, 65.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.58G/3.50G [00:27<00:37, 50.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.76G/9.98G [00:27<02:01, 67.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.59G/3.50G [00:27<00:37, 51.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.77G/9.98G [00:27<02:03, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.78G/9.98G [00:27<02:00, 68.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.60G/3.50G [00:27<00:39, 48.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.79G/9.98G [00:27<02:03, 66.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.61G/3.50G [00:27<00:38, 49.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.80G/9.98G [00:27<01:58, 68.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.81G/9.98G [00:28<02:02, 66.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.63G/3.50G [00:28<00:39, 47.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.82G/9.98G [00:28<02:04, 65.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.64G/3.50G [00:28<00:38, 48.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.84G/9.98G [00:28<01:59, 67.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.65G/3.50G [00:28<00:37, 49.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.85G/9.98G [00:28<02:02, 66.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.66G/3.50G [00:28<00:36, 50.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.86G/9.98G [00:28<02:26, 55.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 1.67G/3.50G [00:29<00:37, 49.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.87G/9.98G [00:29<02:35, 52.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 1.68G/3.50G [00:29<00:37, 48.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.88G/9.98G [00:29<02:27, 54.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.89G/9.98G [00:29<02:22, 57.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 1.69G/3.50G [00:29<00:36, 50.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.90G/9.98G [00:29<02:09, 62.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.70G/3.50G [00:29<00:35, 51.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.91G/9.98G [00:29<02:10, 61.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.71G/3.50G [00:29<00:34, 51.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.92G/9.98G [00:29<02:11, 61.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.72G/3.50G [00:30<00:34, 52.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.93G/9.98G [00:30<02:01, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.94G/9.98G [00:30<02:04, 64.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.73G/3.50G [00:30<00:33, 52.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.95G/9.98G [00:30<02:07, 63.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.74G/3.50G [00:30<00:33, 52.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.96G/9.98G [00:30<01:58, 67.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.75G/3.50G [00:30<00:33, 52.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.97G/9.98G [00:30<02:02, 65.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.76G/3.50G [00:30<00:32, 53.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.98G/9.98G [00:30<01:56, 68.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.99G/9.98G [00:30<02:00, 66.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.77G/3.50G [00:31<00:32, 53.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.00G/9.98G [00:31<01:53, 70.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.78G/3.50G [00:31<00:32, 53.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.01G/9.98G [00:31<01:58, 67.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.79G/3.50G [00:31<00:29, 58.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.02G/9.98G [00:31<02:03, 64.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.80G/3.50G [00:31<00:29, 56.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.03G/9.98G [00:31<01:54, 69.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.81G/3.50G [00:31<00:30, 55.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.04G/9.98G [00:31<01:59, 66.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.06G/9.98G [00:31<01:53, 69.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.82G/3.50G [00:31<00:30, 55.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.07G/9.98G [00:32<01:58, 66.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.84G/3.50G [00:32<00:30, 54.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.08G/9.98G [00:32<02:02, 64.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.85G/3.50G [00:32<00:30, 53.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.09G/9.98G [00:32<01:54, 68.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.10G/9.98G [00:32<01:59, 66.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.86G/3.50G [00:32<00:33, 49.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.11G/9.98G [00:32<02:02, 64.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.87G/3.50G [00:32<00:35, 46.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.12G/9.98G [00:32<01:54, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.13G/9.98G [00:33<01:59, 65.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 1.88G/3.50G [00:33<00:33, 48.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.14G/9.98G [00:33<02:03, 63.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 1.89G/3.50G [00:33<00:35, 45.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.15G/9.98G [00:33<01:55, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.16G/9.98G [00:33<02:00, 65.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 1.90G/3.50G [00:33<00:36, 44.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.17G/9.98G [00:33<02:02, 63.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.91G/3.50G [00:33<00:34, 46.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.18G/9.98G [00:33<02:04, 62.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.19G/9.98G [00:33<01:55, 67.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.92G/3.50G [00:34<00:35, 44.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.20G/9.98G [00:34<02:10, 59.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.93G/3.50G [00:34<00:35, 44.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.21G/9.98G [00:34<02:10, 59.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.94G/3.50G [00:34<00:33, 46.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.22G/9.98G [00:34<02:05, 61.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.23G/9.98G [00:34<02:00, 64.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.95G/3.50G [00:34<00:34, 45.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.24G/9.98G [00:34<01:59, 64.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.96G/3.50G [00:34<00:33, 46.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.25G/9.98G [00:35<02:01, 63.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.97G/3.50G [00:35<00:33, 46.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.26G/9.98G [00:35<01:57, 65.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.28G/9.98G [00:35<01:56, 65.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 1.98G/3.50G [00:35<00:32, 46.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.29G/9.98G [00:35<01:53, 67.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 1.99G/3.50G [00:35<00:31, 48.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.30G/9.98G [00:35<01:57, 65.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 2.00G/3.50G [00:35<00:32, 46.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.31G/9.98G [00:35<01:57, 65.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.32G/9.98G [00:35<01:53, 67.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.01G/3.50G [00:36<00:31, 47.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.33G/9.98G [00:36<01:54, 66.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.02G/3.50G [00:36<00:30, 47.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.34G/9.98G [00:36<01:58, 64.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.35G/9.98G [00:36<01:54, 66.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.03G/3.50G [00:36<00:30, 47.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.36G/9.98G [00:36<01:54, 66.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.04G/3.50G [00:36<00:29, 49.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.37G/9.98G [00:36<01:51, 68.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.06G/3.50G [00:36<00:29, 49.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.38G/9.98G [00:36<01:56, 65.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.39G/9.98G [00:37<01:55, 65.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.07G/3.50G [00:37<00:30, 47.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.40G/9.98G [00:37<01:52, 67.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.08G/3.50G [00:37<00:29, 49.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.41G/9.98G [00:37<01:53, 66.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.09G/3.50G [00:37<00:28, 50.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.42G/9.98G [00:37<01:57, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.43G/9.98G [00:37<01:57, 64.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.10G/3.50G [00:37<00:29, 47.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.44G/9.98G [00:37<01:54, 65.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.11G/3.50G [00:37<00:28, 49.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.45G/9.98G [00:37<01:52, 66.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.12G/3.50G [00:38<00:27, 50.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.46G/9.98G [00:38<01:51, 67.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.47G/9.98G [00:38<01:49, 68.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.13G/3.50G [00:38<00:28, 47.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.49G/9.98G [00:38<01:54, 65.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.14G/3.50G [00:38<00:27, 48.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.50G/9.98G [00:38<01:52, 66.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.15G/3.50G [00:38<00:27, 49.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.51G/9.98G [00:38<01:50, 67.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.52G/9.98G [00:38<01:50, 67.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.16G/3.50G [00:39<00:26, 51.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.53G/9.98G [00:39<01:54, 64.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.17G/3.50G [00:39<00:27, 48.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.54G/9.98G [00:39<01:52, 66.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.18G/3.50G [00:39<00:26, 49.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.55G/9.98G [00:39<01:50, 67.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.56G/9.98G [00:39<01:50, 67.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.19G/3.50G [00:39<00:25, 50.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.57G/9.98G [00:39<01:53, 65.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.20G/3.50G [00:39<00:25, 51.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.58G/9.98G [00:39<01:55, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.59G/9.98G [00:40<01:49, 67.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.21G/3.50G [00:40<00:26, 48.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.60G/9.98G [00:40<01:51, 66.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.22G/3.50G [00:40<00:25, 49.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.61G/9.98G [00:40<01:46, 69.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.23G/3.50G [00:40<00:25, 50.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.62G/9.98G [00:40<01:50, 66.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.24G/3.50G [00:40<00:24, 51.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.63G/9.98G [00:40<01:53, 64.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.64G/9.98G [00:40<01:47, 68.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.25G/3.50G [00:40<00:25, 48.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.65G/9.98G [00:40<01:50, 66.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.26G/3.50G [00:41<00:24, 49.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.66G/9.98G [00:41<01:45, 69.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.28G/3.50G [00:41<00:24, 50.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.67G/9.98G [00:41<01:50, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.68G/9.98G [00:41<01:52, 64.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.29G/3.50G [00:41<00:23, 51.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.69G/9.98G [00:41<01:47, 67.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.30G/3.50G [00:41<00:24, 48.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.71G/9.98G [00:41<01:49, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.72G/9.98G [00:41<01:45, 69.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.31G/3.50G [00:41<00:24, 48.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.73G/9.98G [00:42<01:49, 66.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.32G/3.50G [00:42<00:23, 50.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.74G/9.98G [00:42<01:51, 64.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.33G/3.50G [00:42<00:23, 50.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.75G/9.98G [00:42<01:46, 67.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.34G/3.50G [00:42<00:23, 50.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.76G/9.98G [00:42<01:50, 65.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.77G/9.98G [00:42<01:51, 64.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.35G/3.50G [00:42<00:23, 48.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.78G/9.98G [00:42<01:47, 67.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.36G/3.50G [00:43<00:22, 50.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.79G/9.98G [00:43<01:48, 66.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.37G/3.50G [00:43<00:22, 51.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.80G/9.98G [00:43<01:51, 64.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.38G/3.50G [00:43<00:22, 50.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.81G/9.98G [00:43<02:01, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.82G/9.98G [00:43<02:01, 58.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.39G/3.50G [00:43<00:22, 48.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.83G/9.98G [00:43<01:50, 64.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.40G/3.50G [00:43<00:22, 49.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.84G/9.98G [00:43<01:53, 63.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.41G/3.50G [00:44<00:21, 50.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.85G/9.98G [00:44<01:44, 67.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.42G/3.50G [00:44<00:20, 51.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.86G/9.98G [00:44<01:49, 65.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.87G/9.98G [00:44<01:52, 63.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.43G/3.50G [00:44<00:22, 48.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.88G/9.98G [00:44<01:44, 68.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.44G/3.50G [00:44<00:21, 49.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.89G/9.98G [00:44<01:48, 65.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.45G/3.50G [00:44<00:20, 50.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.90G/9.98G [00:44<01:51, 63.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.92G/9.98G [00:44<01:43, 68.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.46G/3.50G [00:45<00:20, 51.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.93G/9.98G [00:45<01:47, 65.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.47G/3.50G [00:45<00:20, 49.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.94G/9.98G [00:45<01:41, 69.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.95G/9.98G [00:45<01:45, 66.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.49G/3.50G [00:45<00:20, 49.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.96G/9.98G [00:45<01:49, 64.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.50G/3.50G [00:45<00:20, 50.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.97G/9.98G [00:45<01:42, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.98G/9.98G [00:45<01:46, 65.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.51G/3.50G [00:46<00:23, 41.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.99G/9.98G [00:46<01:49, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.00G/9.98G [00:46<01:41, 68.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.53G/3.50G [00:46<00:18, 54.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.01G/9.98G [00:46<01:46, 65.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.54G/3.50G [00:46<00:18, 51.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.02G/9.98G [00:46<01:49, 63.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.55G/3.50G [00:46<00:18, 51.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.03G/9.98G [00:46<01:45, 65.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.04G/9.98G [00:46<01:44, 66.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.56G/3.50G [00:46<00:17, 52.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.05G/9.98G [00:47<01:41, 67.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.57G/3.50G [00:47<00:17, 52.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.06G/9.98G [00:47<01:45, 65.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.58G/3.50G [00:47<00:17, 53.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.07G/9.98G [00:47<01:45, 65.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.59G/3.50G [00:47<00:17, 52.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.08G/9.98G [00:47<01:42, 67.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.09G/9.98G [00:47<01:43, 66.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.60G/3.50G [00:47<00:17, 50.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.10G/9.98G [00:47<01:40, 68.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.61G/3.50G [00:48<00:17, 50.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.11G/9.98G [00:48<01:44, 65.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.62G/3.50G [00:48<00:17, 51.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.12G/9.98G [00:48<01:44, 65.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.14G/9.98G [00:48<01:40, 68.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.63G/3.50G [00:48<00:16, 51.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.15G/9.98G [00:48<01:41, 67.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.64G/3.50G [00:48<00:16, 52.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.16G/9.98G [00:48<01:45, 64.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.65G/3.50G [00:48<00:16, 52.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.17G/9.98G [00:48<01:41, 67.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.66G/3.50G [00:48<00:15, 52.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.18G/9.98G [00:48<01:42, 66.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.19G/9.98G [00:49<01:38, 68.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.67G/3.50G [00:49<00:15, 52.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.20G/9.98G [00:49<01:43, 65.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.68G/3.50G [00:49<00:16, 49.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.21G/9.98G [00:49<01:44, 65.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.22G/9.98G [00:49<01:39, 68.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.69G/3.50G [00:49<00:15, 50.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.23G/9.98G [00:49<01:41, 66.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.71G/3.50G [00:49<00:15, 51.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.24G/9.98G [00:49<01:44, 64.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.72G/3.50G [00:50<00:14, 52.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.25G/9.98G [00:50<01:39, 67.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.73G/3.50G [00:50<00:14, 53.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.26G/9.98G [00:50<01:41, 66.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.74G/3.50G [00:50<00:13, 56.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.27G/9.98G [00:50<01:37, 68.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.75G/3.50G [00:50<00:13, 55.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.28G/9.98G [00:50<01:40, 66.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.29G/9.98G [00:50<01:43, 64.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.76G/3.50G [00:50<00:13, 54.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.30G/9.98G [00:50<01:36, 68.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.77G/3.50G [00:50<00:13, 54.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.31G/9.98G [00:50<01:40, 66.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.78G/3.50G [00:51<00:13, 54.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.32G/9.98G [00:51<01:35, 69.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.33G/9.98G [00:51<01:39, 66.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 2.79G/3.50G [00:51<00:13, 53.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.34G/9.98G [00:51<01:42, 64.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 2.80G/3.50G [00:51<00:13, 53.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.36G/9.98G [00:51<01:37, 68.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 2.81G/3.50G [00:51<00:12, 53.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.37G/9.98G [00:51<01:40, 65.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.82G/3.50G [00:51<00:12, 53.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.38G/9.98G [00:51<01:34, 69.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.83G/3.50G [00:52<00:11, 58.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.39G/9.98G [00:52<01:39, 66.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.84G/3.50G [00:52<00:11, 57.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.40G/9.98G [00:52<01:42, 64.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.85G/3.50G [00:52<00:11, 56.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.41G/9.98G [00:52<01:44, 62.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.42G/9.98G [00:52<01:37, 67.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.86G/3.50G [00:52<00:11, 55.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.43G/9.98G [00:52<01:41, 64.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.87G/3.50G [00:52<00:10, 59.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.44G/9.98G [00:52<01:34, 69.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.88G/3.50G [00:53<00:10, 57.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.45G/9.98G [00:53<01:38, 66.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.89G/3.50G [00:53<00:10, 56.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.46G/9.98G [00:53<01:41, 64.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.47G/9.98G [00:53<01:34, 68.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.90G/3.50G [00:53<00:10, 54.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.92G/3.50G [00:53<00:09, 59.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.48G/9.98G [00:53<01:38, 65.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.49G/9.98G [00:53<01:32, 70.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.93G/3.50G [00:53<00:09, 57.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.50G/9.98G [00:53<01:38, 65.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.94G/3.50G [00:53<00:09, 56.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.51G/9.98G [00:53<01:40, 64.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.95G/3.50G [00:54<00:09, 61.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.52G/9.98G [00:54<01:42, 63.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.96G/3.50G [00:54<00:09, 58.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.53G/9.98G [00:54<01:35, 67.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 2.97G/3.50G [00:54<00:09, 57.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.54G/9.98G [00:54<01:38, 65.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 2.98G/3.50G [00:54<00:08, 61.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.55G/9.98G [00:54<01:32, 69.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 2.99G/3.50G [00:54<00:08, 58.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.57G/9.98G [00:54<01:36, 66.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.00G/3.50G [00:54<00:08, 57.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.58G/9.98G [00:54<01:39, 64.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.01G/3.50G [00:55<00:07, 61.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.59G/9.98G [00:55<01:33, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.60G/9.98G [00:55<01:37, 65.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.02G/3.50G [00:55<00:08, 58.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.61G/9.98G [00:55<01:39, 63.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.03G/3.50G [00:55<00:08, 57.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.62G/9.98G [00:55<01:34, 67.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.04G/3.50G [00:55<00:07, 61.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.63G/9.98G [00:55<01:36, 66.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.05G/3.50G [00:55<00:07, 58.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.64G/9.98G [00:55<01:31, 69.2MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.06G/3.50G [00:56<00:07, 57.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.65G/9.98G [00:56<01:36, 65.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.07G/3.50G [00:56<00:06, 61.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.66G/9.98G [00:56<01:39, 63.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.08G/3.50G [00:56<00:07, 58.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.67G/9.98G [00:56<01:32, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.68G/9.98G [00:56<01:35, 65.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.09G/3.50G [00:56<00:07, 57.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.10G/3.50G [00:56<00:06, 61.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.69G/9.98G [00:56<02:03, 50.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.11G/3.50G [00:56<00:06, 59.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.71G/9.98G [00:57<01:29, 70.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.12G/3.50G [00:57<00:05, 62.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.72G/9.98G [00:57<01:27, 71.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.14G/3.50G [00:57<00:06, 60.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.73G/9.98G [00:57<01:30, 68.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.15G/3.50G [00:57<00:06, 58.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.74G/9.98G [00:57<01:31, 68.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.16G/3.50G [00:57<00:06, 56.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.75G/9.98G [00:57<01:30, 68.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.17G/3.50G [00:57<00:05, 60.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.76G/9.98G [00:57<01:34, 65.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.18G/3.50G [00:57<00:05, 58.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.77G/9.98G [00:57<01:33, 66.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.19G/3.50G [00:58<00:04, 62.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.79G/9.98G [00:58<01:31, 67.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.20G/3.50G [00:58<00:05, 59.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.80G/9.98G [00:58<01:34, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.81G/9.98G [00:58<01:34, 65.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.21G/3.50G [00:58<00:05, 57.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.82G/9.98G [00:58<01:31, 67.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.22G/3.50G [00:58<00:04, 61.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.83G/9.98G [00:58<01:31, 67.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.23G/3.50G [00:58<00:04, 59.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.84G/9.98G [00:58<01:34, 64.8MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.24G/3.50G [00:59<00:04, 57.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.85G/9.98G [00:59<01:38, 62.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.25G/3.50G [00:59<00:04, 61.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.86G/9.98G [00:59<01:38, 62.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.26G/3.50G [00:59<00:04, 58.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.87G/9.98G [00:59<01:31, 66.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.27G/3.50G [00:59<00:04, 57.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.88G/9.98G [00:59<01:35, 64.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.28G/3.50G [00:59<00:03, 61.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.89G/9.98G [00:59<01:37, 62.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.29G/3.50G [00:59<00:03, 58.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.90G/9.98G [00:59<01:38, 61.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.91G/9.98G [01:00<01:30, 66.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.30G/3.50G [01:00<00:03, 57.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.92G/9.98G [01:00<01:33, 64.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.31G/3.50G [01:00<00:03, 55.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.32G/3.50G [01:00<00:02, 60.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.93G/9.98G [01:00<01:36, 62.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.94G/9.98G [01:00<01:29, 67.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.33G/3.50G [01:00<00:02, 58.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.95G/9.98G [01:00<01:32, 64.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.34G/3.50G [01:00<00:02, 62.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.96G/9.98G [01:00<01:26, 69.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.36G/3.50G [01:00<00:02, 59.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.97G/9.98G [01:01<01:30, 66.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.37G/3.50G [01:01<00:02, 57.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.98G/9.98G [01:01<01:33, 63.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.38G/3.50G [01:01<00:02, 61.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.00G/9.98G [01:01<01:27, 68.0MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.39G/3.50G [01:01<00:01, 59.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.01G/9.98G [01:01<01:32, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.02G/9.98G [01:01<01:26, 68.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.40G/3.50G [01:01<00:01, 57.1MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.41G/3.50G [01:01<00:01, 61.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.03G/9.98G [01:01<01:30, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.04G/9.98G [01:01<01:33, 63.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.42G/3.50G [01:02<00:01, 58.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.05G/9.98G [01:02<01:26, 68.4MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.43G/3.50G [01:02<00:01, 57.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.06G/9.98G [01:02<01:30, 65.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.44G/3.50G [01:02<00:00, 61.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.07G/9.98G [01:02<01:24, 69.6MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.45G/3.50G [01:02<00:00, 59.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.08G/9.98G [01:02<01:28, 66.5MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.46G/3.50G [01:02<00:00, 57.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.09G/9.98G [01:02<01:31, 64.1MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.47G/3.50G [01:02<00:00, 62.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.10G/9.98G [01:02<01:26, 68.3MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.48G/3.50G [01:03<00:00, 58.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.11G/9.98G [01:03<01:28, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.12G/9.98G [01:03<01:23, 69.7MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.49G/3.50G [01:03<00:00, 57.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.13G/9.98G [01:03<01:28, 65.9MB/s]\u001b[A\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.50G/3.50G [01:03<00:00, 55.2MB/s]\n",
            "\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.14G/9.98G [01:03<01:30, 64.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.15G/9.98G [01:03<01:24, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.16G/9.98G [01:03<01:27, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.17G/9.98G [01:04<01:30, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.18G/9.98G [01:04<01:25, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.19G/9.98G [01:04<01:28, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.20G/9.98G [01:04<01:23, 69.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.22G/9.98G [01:04<01:26, 66.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.23G/9.98G [01:04<01:29, 64.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.24G/9.98G [01:04<01:31, 62.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.25G/9.98G [01:05<01:54, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.26G/9.98G [01:05<01:48, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.27G/9.98G [01:05<01:45, 54.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.28G/9.98G [01:05<01:42, 55.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.29G/9.98G [01:06<01:41, 56.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.30G/9.98G [01:06<01:31, 62.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.31G/9.98G [01:06<01:32, 61.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.32G/9.98G [01:06<01:24, 66.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.33G/9.98G [01:06<01:27, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.34G/9.98G [01:06<01:29, 62.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.35G/9.98G [01:06<01:22, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.36G/9.98G [01:07<01:26, 65.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.37G/9.98G [01:07<01:20, 69.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.38G/9.98G [01:07<01:24, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.39G/9.98G [01:07<01:26, 64.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.40G/9.98G [01:07<01:20, 68.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.41G/9.98G [01:07<01:24, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.42G/9.98G [01:08<01:19, 70.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.44G/9.98G [01:08<01:23, 66.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.45G/9.98G [01:08<01:18, 70.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.46G/9.98G [01:08<01:22, 66.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.47G/9.98G [01:08<01:25, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.48G/9.98G [01:08<01:20, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.49G/9.98G [01:08<01:23, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.50G/9.98G [01:09<01:25, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.51G/9.98G [01:09<01:21, 66.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.52G/9.98G [01:09<01:26, 62.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.53G/9.98G [01:09<01:28, 61.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.54G/9.98G [01:09<01:29, 60.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.55G/9.98G [01:09<01:21, 66.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.56G/9.98G [01:10<01:24, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.57G/9.98G [01:10<01:18, 69.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.58G/9.98G [01:10<01:22, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.59G/9.98G [01:10<01:24, 63.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.60G/9.98G [01:10<01:18, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.61G/9.98G [01:10<01:21, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.62G/9.98G [01:11<01:24, 63.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.63G/9.98G [01:11<01:18, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.65G/9.98G [01:11<01:22, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.66G/9.98G [01:11<01:17, 68.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.67G/9.98G [01:11<01:20, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.68G/9.98G [01:11<01:23, 63.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.69G/9.98G [01:12<01:17, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.70G/9.98G [01:12<01:21, 64.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.71G/9.98G [01:12<01:23, 63.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.72G/9.98G [01:12<01:17, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.73G/9.98G [01:12<01:20, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.74G/9.98G [01:12<01:15, 69.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.75G/9.98G [01:12<01:18, 66.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.76G/9.98G [01:13<01:21, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.77G/9.98G [01:13<01:15, 68.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.78G/9.98G [01:13<01:19, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.79G/9.98G [01:13<01:22, 63.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.80G/9.98G [01:13<01:16, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.81G/9.98G [01:13<01:19, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.82G/9.98G [01:14<01:13, 69.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.83G/9.98G [01:14<01:17, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.84G/9.98G [01:14<01:20, 63.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.85G/9.98G [01:14<01:15, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.87G/9.98G [01:14<01:17, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.88G/9.98G [01:14<01:20, 63.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.89G/9.98G [01:15<01:14, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.90G/9.98G [01:15<01:17, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.91G/9.98G [01:15<01:19, 63.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.92G/9.98G [01:15<01:14, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.93G/9.98G [01:15<01:17, 64.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.94G/9.98G [01:15<01:12, 69.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.95G/9.98G [01:15<01:15, 66.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.96G/9.98G [01:16<01:18, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.97G/9.98G [01:16<01:12, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.98G/9.98G [01:16<01:24, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.99G/9.98G [01:16<01:31, 54.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.00G/9.98G [01:16<01:29, 55.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.01G/9.98G [01:17<01:34, 52.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.02G/9.98G [01:17<01:31, 54.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.03G/9.98G [01:17<01:34, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.04G/9.98G [01:17<01:32, 53.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.05G/9.98G [01:17<01:35, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.06G/9.98G [01:18<01:32, 53.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.08G/9.98G [01:18<01:29, 54.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.09G/9.98G [01:18<01:27, 56.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.10G/9.98G [01:18<01:31, 53.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.11G/9.98G [01:18<01:30, 53.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.12G/9.98G [01:19<01:27, 55.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.13G/9.98G [01:19<01:25, 56.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.14G/9.98G [01:19<01:24, 57.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.15G/9.98G [01:19<01:30, 53.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.16G/9.98G [01:19<01:27, 54.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.17G/9.98G [01:20<01:25, 56.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.18G/9.98G [01:20<01:24, 56.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.19G/9.98G [01:20<01:23, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.20G/9.98G [01:20<01:22, 58.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.21G/9.98G [01:20<01:21, 58.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.22G/9.98G [01:20<01:21, 58.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.23G/9.98G [01:21<01:20, 58.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.24G/9.98G [01:21<01:20, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.25G/9.98G [01:21<01:20, 58.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.26G/9.98G [01:21<01:19, 59.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.27G/9.98G [01:21<01:19, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.28G/9.98G [01:21<01:23, 56.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.30G/9.98G [01:22<01:23, 56.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.31G/9.98G [01:22<01:21, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.32G/9.98G [01:22<01:19, 58.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.33G/9.98G [01:22<01:18, 59.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.34G/9.98G [01:22<01:15, 61.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.35G/9.98G [01:23<01:22, 55.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.36G/9.98G [01:23<01:20, 57.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.37G/9.98G [01:23<01:13, 62.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.38G/9.98G [01:23<01:15, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.39G/9.98G [01:23<01:19, 57.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.40G/9.98G [01:23<01:18, 58.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.41G/9.98G [01:24<01:17, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.42G/9.98G [01:24<01:16, 59.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.43G/9.98G [01:24<01:15, 59.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.44G/9.98G [01:24<01:16, 59.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.45G/9.98G [01:24<01:15, 59.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.46G/9.98G [01:25<01:17, 57.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.47G/9.98G [01:25<01:16, 58.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.48G/9.98G [01:25<01:38, 45.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.51G/9.98G [01:25<01:09, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.52G/9.98G [01:25<01:10, 62.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.53G/9.98G [01:26<01:11, 62.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.54G/9.98G [01:26<01:12, 61.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.55G/9.98G [01:26<01:13, 60.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.56G/9.98G [01:26<01:13, 60.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.57G/9.98G [01:26<01:13, 59.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.58G/9.98G [01:26<01:15, 58.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.59G/9.98G [01:27<01:19, 55.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.60G/9.98G [01:27<01:25, 51.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.61G/9.98G [01:27<01:28, 49.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.62G/9.98G [01:27<01:30, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.63G/9.98G [01:28<01:27, 49.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.64G/9.98G [01:28<01:28, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.65G/9.98G [01:28<01:30, 48.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.66G/9.98G [01:28<01:30, 47.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.67G/9.98G [01:28<01:26, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.68G/9.98G [01:29<01:27, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.69G/9.98G [01:29<01:27, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.70G/9.98G [01:29<01:25, 49.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.71G/9.98G [01:29<01:25, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.73G/9.98G [01:29<01:22, 51.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.74G/9.98G [01:30<01:23, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.75G/9.98G [01:30<01:21, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.76G/9.98G [01:30<01:21, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.77G/9.98G [01:30<01:20, 52.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.78G/9.98G [01:30<01:18, 53.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.79G/9.98G [01:31<01:19, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.80G/9.98G [01:31<01:18, 53.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.81G/9.98G [01:31<01:17, 54.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.82G/9.98G [01:31<01:18, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.83G/9.98G [01:31<01:16, 54.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.84G/9.98G [01:32<01:18, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.85G/9.98G [01:32<01:17, 53.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.86G/9.98G [01:32<01:14, 55.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.87G/9.98G [01:32<01:13, 56.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.88G/9.98G [01:32<01:14, 55.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.89G/9.98G [01:33<01:14, 54.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.90G/9.98G [01:33<01:12, 56.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.91G/9.98G [01:33<01:11, 56.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.92G/9.98G [01:33<01:10, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.93G/9.98G [01:33<01:09, 58.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.95G/9.98G [01:33<01:08, 58.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.96G/9.98G [01:34<01:08, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.97G/9.98G [01:34<01:08, 58.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.98G/9.98G [01:34<01:07, 59.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.99G/9.98G [01:34<01:07, 58.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.00G/9.98G [01:34<01:07, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.01G/9.98G [01:35<01:12, 55.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.02G/9.98G [01:35<01:16, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.03G/9.98G [01:35<01:19, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.04G/9.98G [01:35<01:21, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.05G/9.98G [01:35<01:23, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.06G/9.98G [01:36<01:23, 46.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.07G/9.98G [01:36<01:24, 46.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.08G/9.98G [01:36<01:24, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.09G/9.98G [01:36<01:24, 46.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.10G/9.98G [01:37<01:24, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.11G/9.98G [01:37<01:24, 45.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.12G/9.98G [01:37<01:18, 48.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.13G/9.98G [01:37<01:19, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.14G/9.98G [01:38<01:20, 47.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.16G/9.98G [01:38<01:21, 47.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.17G/9.98G [01:38<01:16, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.18G/9.98G [01:38<01:17, 48.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.19G/9.98G [01:38<01:19, 47.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.20G/9.98G [01:39<01:15, 50.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.21G/9.98G [01:39<01:16, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.22G/9.98G [01:39<01:18, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.23G/9.98G [01:39<01:13, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.24G/9.98G [01:39<01:15, 49.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.25G/9.98G [01:40<01:15, 49.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.26G/9.98G [01:40<01:13, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.27G/9.98G [01:40<01:15, 49.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.28G/9.98G [01:40<01:11, 51.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.29G/9.98G [01:40<01:14, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.30G/9.98G [01:41<01:13, 50.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.31G/9.98G [01:41<01:12, 50.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.32G/9.98G [01:41<01:11, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.33G/9.98G [01:41<01:11, 51.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.34G/9.98G [01:41<01:11, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.35G/9.98G [01:42<01:10, 51.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.36G/9.98G [01:42<01:12, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.38G/9.98G [01:42<01:09, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.39G/9.98G [01:42<01:11, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.40G/9.98G [01:43<01:09, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.41G/9.98G [01:43<01:10, 50.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.42G/9.98G [01:43<01:08, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.43G/9.98G [01:43<01:09, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.44G/9.98G [01:43<01:07, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.45G/9.98G [01:44<01:09, 50.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.46G/9.98G [01:44<01:06, 52.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.47G/9.98G [01:44<01:08, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.48G/9.98G [01:44<01:06, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.49G/9.98G [01:44<01:08, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.50G/9.98G [01:45<01:08, 50.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.51G/9.98G [01:45<01:07, 51.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.52G/9.98G [01:45<01:07, 51.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.53G/9.98G [01:45<01:06, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.54G/9.98G [01:45<01:06, 51.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.55G/9.98G [01:46<01:06, 51.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.56G/9.98G [01:46<01:09, 49.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.57G/9.98G [01:46<01:16, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.59G/9.98G [01:46<01:20, 41.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.60G/9.98G [01:47<01:40, 33.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.62G/9.98G [01:47<01:18, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.63G/9.98G [01:47<01:18, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.64G/9.98G [01:48<01:17, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.65G/9.98G [01:48<01:16, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.66G/9.98G [01:48<01:17, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.67G/9.98G [01:48<01:16, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.68G/9.98G [01:49<01:15, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.69G/9.98G [01:49<01:14, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.70G/9.98G [01:49<01:12, 44.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.71G/9.98G [01:49<01:10, 46.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.72G/9.98G [01:49<01:09, 46.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.73G/9.98G [01:50<01:09, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.74G/9.98G [01:50<01:09, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.75G/9.98G [01:50<01:09, 46.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.76G/9.98G [01:50<01:07, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.77G/9.98G [01:51<01:05, 48.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.78G/9.98G [01:51<01:06, 48.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.79G/9.98G [01:51<01:06, 47.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.81G/9.98G [01:51<01:04, 49.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.82G/9.98G [01:51<01:04, 49.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.83G/9.98G [01:52<01:05, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.84G/9.98G [01:52<01:02, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.85G/9.98G [01:52<01:03, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.86G/9.98G [01:52<01:02, 50.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.87G/9.98G [01:52<01:01, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.88G/9.98G [01:53<01:03, 49.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.89G/9.98G [01:53<01:00, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.90G/9.98G [01:53<01:01, 50.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.91G/9.98G [01:53<01:00, 51.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.92G/9.98G [01:54<01:00, 50.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.93G/9.98G [01:54<00:59, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.94G/9.98G [01:54<00:59, 51.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.95G/9.98G [01:54<01:01, 49.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.96G/9.98G [01:54<00:58, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.97G/9.98G [01:55<01:00, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.98G/9.98G [01:55<00:58, 51.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.99G/9.98G [01:55<00:58, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.00G/9.98G [01:55<00:57, 51.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.01G/9.98G [01:55<00:58, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.03G/9.98G [01:56<00:56, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.04G/9.98G [01:56<00:57, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.05G/9.98G [01:56<00:56, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.06G/9.98G [01:56<00:57, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.07G/9.98G [01:56<00:55, 52.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.08G/9.98G [01:57<00:56, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.09G/9.98G [01:57<00:55, 52.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.10G/9.98G [01:57<00:56, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.11G/9.98G [01:57<00:57, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.12G/9.98G [01:57<00:55, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.13G/9.98G [01:58<00:56, 50.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.14G/9.98G [01:58<00:54, 52.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.15G/9.98G [01:58<00:55, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.16G/9.98G [01:58<00:53, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.17G/9.98G [01:59<01:08, 40.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.19G/9.98G [01:59<00:51, 54.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.20G/9.98G [01:59<00:50, 55.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.21G/9.98G [01:59<00:52, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.22G/9.98G [01:59<00:51, 53.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.24G/9.98G [02:00<00:53, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.25G/9.98G [02:00<00:51, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.26G/9.98G [02:00<00:53, 51.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.27G/9.98G [02:00<00:51, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.28G/9.98G [02:00<00:52, 51.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.29G/9.98G [02:01<00:51, 52.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.30G/9.98G [02:01<00:52, 51.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.31G/9.98G [02:01<00:51, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.32G/9.98G [02:01<00:51, 51.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.33G/9.98G [02:01<00:50, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.34G/9.98G [02:02<00:51, 51.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.35G/9.98G [02:02<00:50, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.36G/9.98G [02:02<00:50, 51.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.37G/9.98G [02:02<00:49, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.38G/9.98G [02:02<00:50, 51.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.39G/9.98G [02:03<00:48, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.40G/9.98G [02:03<00:50, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.41G/9.98G [02:03<00:48, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.42G/9.98G [02:03<00:47, 53.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.43G/9.98G [02:03<00:48, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.44G/9.98G [02:04<00:47, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.46G/9.98G [02:04<00:48, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.47G/9.98G [02:04<00:47, 53.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.48G/9.98G [02:04<00:45, 54.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.49G/9.98G [02:04<00:47, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.50G/9.98G [02:05<00:46, 53.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.51G/9.98G [02:05<00:47, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.52G/9.98G [02:05<00:45, 53.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.53G/9.98G [02:05<00:44, 54.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.54G/9.98G [02:05<00:46, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.55G/9.98G [02:06<00:45, 53.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.56G/9.98G [02:06<00:46, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.57G/9.98G [02:06<00:44, 53.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.58G/9.98G [02:06<00:46, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.59G/9.98G [02:06<00:44, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.60G/9.98G [02:07<00:43, 54.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.61G/9.98G [02:07<00:42, 56.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.62G/9.98G [02:07<00:41, 57.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.63G/9.98G [02:07<00:40, 57.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.64G/9.98G [02:07<00:40, 58.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.65G/9.98G [02:07<00:39, 58.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.67G/9.98G [02:08<00:39, 58.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.68G/9.98G [02:08<00:38, 59.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.69G/9.98G [02:08<00:38, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.70G/9.98G [02:08<00:38, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.71G/9.98G [02:08<00:36, 61.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.72G/9.98G [02:09<00:36, 61.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.73G/9.98G [02:09<00:36, 62.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.74G/9.98G [02:09<00:36, 61.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.75G/9.98G [02:09<00:36, 61.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.76G/9.98G [02:09<00:34, 64.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.77G/9.98G [02:09<00:33, 64.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.78G/9.98G [02:10<00:34, 63.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.79G/9.98G [02:10<00:33, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.80G/9.98G [02:10<00:32, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.81G/9.98G [02:10<00:33, 63.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.82G/9.98G [02:10<00:32, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.83G/9.98G [02:10<00:32, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.84G/9.98G [02:10<00:33, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.85G/9.98G [02:11<00:31, 66.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.86G/9.98G [02:11<00:31, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.87G/9.98G [02:11<00:30, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.89G/9.98G [02:11<00:31, 67.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.90G/9.98G [02:11<00:32, 64.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.91G/9.98G [02:11<00:30, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.92G/9.98G [02:12<00:30, 66.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.93G/9.98G [02:12<00:31, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.94G/9.98G [02:12<00:30, 67.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.95G/9.98G [02:12<00:30, 65.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.96G/9.98G [02:12<00:29, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.97G/9.98G [02:12<00:30, 65.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.98G/9.98G [02:13<00:30, 65.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.99G/9.98G [02:13<00:29, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.00G/9.98G [02:13<00:29, 66.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.01G/9.98G [02:13<00:28, 68.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.02G/9.98G [02:13<00:29, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.03G/9.98G [02:13<00:29, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.04G/9.98G [02:13<00:28, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.05G/9.98G [02:14<00:29, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.06G/9.98G [02:14<00:29, 64.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.07G/9.98G [02:14<00:28, 67.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.08G/9.98G [02:14<00:28, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.10G/9.98G [02:14<00:27, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.11G/9.98G [02:14<00:28, 65.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.12G/9.98G [02:15<00:28, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.13G/9.98G [02:15<00:27, 66.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.14G/9.98G [02:15<00:28, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.15G/9.98G [02:15<00:28, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.16G/9.98G [02:15<00:27, 67.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.17G/9.98G [02:15<00:27, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.18G/9.98G [02:15<00:26, 68.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.19G/9.98G [02:16<00:27, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.20G/9.98G [02:16<00:27, 64.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.21G/9.98G [02:16<00:26, 67.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.22G/9.98G [02:16<00:26, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.23G/9.98G [02:16<00:27, 64.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.24G/9.98G [02:16<00:25, 67.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.25G/9.98G [02:17<00:25, 66.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.26G/9.98G [02:17<00:25, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.27G/9.98G [02:17<00:26, 65.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.28G/9.98G [02:17<00:26, 64.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.29G/9.98G [02:17<00:24, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.30G/9.98G [02:17<00:25, 65.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.32G/9.98G [02:18<00:25, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.33G/9.98G [02:18<00:24, 67.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.34G/9.98G [02:18<00:24, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.35G/9.98G [02:18<00:23, 69.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.36G/9.98G [02:18<00:24, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.37G/9.98G [02:18<00:24, 64.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.38G/9.98G [02:19<00:23, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.39G/9.98G [02:19<00:23, 66.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.40G/9.98G [02:19<00:24, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.41G/9.98G [02:19<00:23, 67.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.42G/9.98G [02:19<00:23, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.43G/9.98G [02:19<00:22, 69.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.44G/9.98G [02:19<00:22, 67.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.45G/9.98G [02:20<00:21, 69.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.46G/9.98G [02:20<00:22, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.47G/9.98G [02:20<00:22, 65.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.48G/9.98G [02:20<00:21, 68.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.49G/9.98G [02:20<00:22, 67.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.50G/9.98G [02:20<00:21, 69.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.51G/9.98G [02:21<00:21, 67.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.52G/9.98G [02:21<00:22, 64.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.54G/9.98G [02:21<00:21, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.55G/9.98G [02:21<00:21, 66.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.56G/9.98G [02:21<00:20, 69.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.57G/9.98G [02:21<00:23, 59.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.58G/9.98G [02:22<00:26, 53.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.59G/9.98G [02:22<00:23, 59.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.60G/9.98G [02:22<00:23, 59.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.61G/9.98G [02:22<00:25, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.62G/9.98G [02:22<00:22, 59.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.63G/9.98G [02:22<00:22, 58.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.64G/9.98G [02:23<00:22, 58.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.65G/9.98G [02:23<00:20, 64.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.66G/9.98G [02:23<00:20, 63.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.67G/9.98G [02:23<00:21, 61.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.68G/9.98G [02:23<00:19, 65.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.69G/9.98G [02:23<00:20, 63.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.70G/9.98G [02:24<00:18, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.71G/9.98G [02:24<00:19, 65.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.72G/9.98G [02:24<00:19, 63.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.73G/9.98G [02:24<00:18, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.75G/9.98G [02:24<00:19, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.76G/9.98G [02:24<00:19, 62.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.77G/9.98G [02:25<00:17, 67.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.78G/9.98G [02:25<00:18, 64.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.79G/9.98G [02:25<00:17, 69.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.80G/9.98G [02:25<00:17, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.81G/9.98G [02:25<00:18, 63.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.82G/9.98G [02:25<00:16, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.83G/9.98G [02:26<00:17, 65.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.84G/9.98G [02:26<00:17, 63.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.85G/9.98G [02:26<00:16, 67.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.86G/9.98G [02:26<00:17, 65.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.87G/9.98G [02:26<00:15, 69.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.88G/9.98G [02:26<00:16, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.89G/9.98G [02:26<00:17, 63.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.90G/9.98G [02:27<00:17, 62.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.91G/9.98G [02:27<00:15, 67.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.92G/9.98G [02:27<00:16, 64.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.93G/9.98G [02:27<00:15, 69.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.94G/9.98G [02:27<00:15, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.95G/9.98G [02:27<00:15, 64.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.97G/9.98G [02:28<00:14, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.98G/9.98G [02:28<00:15, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.99G/9.98G [02:28<00:14, 69.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.00G/9.98G [02:28<00:15, 63.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.01G/9.98G [02:28<00:14, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.02G/9.98G [02:28<00:14, 65.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.03G/9.98G [02:29<00:14, 63.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.04G/9.98G [02:29<00:14, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.05G/9.98G [02:29<00:14, 65.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.06G/9.98G [02:29<00:13, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.07G/9.98G [02:29<00:13, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.08G/9.98G [02:29<00:13, 64.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.09G/9.98G [02:29<00:13, 67.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.10G/9.98G [02:30<00:13, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.11G/9.98G [02:30<00:12, 69.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.12G/9.98G [02:30<00:12, 67.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.13G/9.98G [02:30<00:13, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.14G/9.98G [02:30<00:12, 67.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.15G/9.98G [02:30<00:12, 65.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.16G/9.98G [02:31<00:11, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.18G/9.98G [02:31<00:12, 65.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.19G/9.98G [02:31<00:12, 63.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.20G/9.98G [02:31<00:11, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.21G/9.98G [02:31<00:11, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.22G/9.98G [02:31<00:12, 63.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.23G/9.98G [02:32<00:11, 67.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.24G/9.98G [02:32<00:11, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.25G/9.98G [02:32<00:11, 62.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.26G/9.98G [02:32<00:11, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.27G/9.98G [02:32<00:11, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.28G/9.98G [02:32<00:10, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.29G/9.98G [02:33<00:10, 64.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.30G/9.98G [02:33<00:10, 63.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.31G/9.98G [02:33<00:10, 63.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.32G/9.98G [02:33<00:10, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.33G/9.98G [02:33<00:09, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.34G/9.98G [02:33<00:09, 64.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.35G/9.98G [02:34<00:09, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.36G/9.98G [02:34<00:09, 65.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.37G/9.98G [02:34<00:08, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.38G/9.98G [02:34<00:08, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.40G/9.98G [02:34<00:08, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.41G/9.98G [02:34<00:08, 67.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.42G/9.98G [02:34<00:08, 65.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.43G/9.98G [02:35<00:08, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.44G/9.98G [02:35<00:08, 66.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.45G/9.98G [02:35<00:08, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.46G/9.98G [02:35<00:07, 71.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.47G/9.98G [02:35<00:07, 66.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.48G/9.98G [02:35<00:07, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.49G/9.98G [02:36<00:07, 69.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.50G/9.98G [02:36<00:07, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.51G/9.98G [02:36<00:06, 70.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.52G/9.98G [02:36<00:06, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.53G/9.98G [02:36<00:06, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.54G/9.98G [02:36<00:06, 69.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.55G/9.98G [02:36<00:06, 65.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.56G/9.98G [02:37<00:06, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.57G/9.98G [02:37<00:05, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.58G/9.98G [02:37<00:05, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.59G/9.98G [02:37<00:06, 63.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.60G/9.98G [02:37<00:05, 68.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.62G/9.98G [02:37<00:05, 65.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.63G/9.98G [02:38<00:05, 63.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.64G/9.98G [02:38<00:04, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.65G/9.98G [02:38<00:05, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.66G/9.98G [02:38<00:04, 70.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.67G/9.98G [02:38<00:04, 66.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.68G/9.98G [02:38<00:04, 64.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.69G/9.98G [02:39<00:04, 68.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.70G/9.98G [02:39<00:04, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.71G/9.98G [02:39<00:03, 69.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.72G/9.98G [02:39<00:03, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.73G/9.98G [02:39<00:03, 64.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.74G/9.98G [02:39<00:03, 68.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.75G/9.98G [02:39<00:03, 65.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.76G/9.98G [02:40<00:03, 69.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.77G/9.98G [02:40<00:03, 66.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.78G/9.98G [02:40<00:03, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.79G/9.98G [02:40<00:02, 68.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.80G/9.98G [02:40<00:02, 64.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.81G/9.98G [02:40<00:02, 63.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.83G/9.98G [02:41<00:02, 67.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.84G/9.98G [02:41<00:02, 65.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.85G/9.98G [02:41<00:01, 69.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.86G/9.98G [02:41<00:01, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.87G/9.98G [02:41<00:01, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.88G/9.98G [02:41<00:01, 68.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.89G/9.98G [02:42<00:01, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.90G/9.98G [02:42<00:01, 69.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.91G/9.98G [02:42<00:01, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.92G/9.98G [02:42<00:00, 64.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.93G/9.98G [02:42<00:00, 68.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.94G/9.98G [02:42<00:00, 65.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.95G/9.98G [02:42<00:00, 70.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.96G/9.98G [02:43<00:00, 66.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.98G/9.98G [02:43<00:00, 61.1MB/s]\n",
            "Fetching 11 files: 100% 11/11 [02:44<00:00, 14.92s/it]\n",
            "Loading ckpt pytorch_model-00001-of-00002.bin\n",
            "Merging...\n",
            "Converting to pth format...\n",
            "Saving shard 1 of 1 into alpaca-combined/L1-consolidated.00.pth\n",
            "Loading ckpt pytorch_model-00002-of-00002.bin\n",
            "Merging...\n",
            "Converting to pth format...\n",
            "Saving shard 1 of 1 into alpaca-combined/L2-consolidated.00.pth\n",
            "Saving tokenizer\n",
            "Saving params.json into alpaca-combined/params.json\n",
            "Loading ['L1-consolidated.00.pth', 'L2-consolidated.00.pth'] ...\n",
            "Saving the merged shard to alpaca-combined/consolidated.00.pth\n",
            "Cleaning up...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 比对SHA256\n",
        "\n",
        "完整值：https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/SHA256.md\n",
        "\n",
        "其中本示例生成的Alpaca-7B的标准SHA256：\n",
        "- fbfccc91183169842aac8d093379f0a449b5a26c5ee7a298baf0d556f1499b90\n",
        "\n",
        "使用下述命令评测后发现两者相同，合并无误。"
      ],
      "metadata": {
        "id": "iO6f_kZOPB_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sha256sum alpaca-combined/consolidated.*.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5u4QDNZPYI_",
        "outputId": "18aa3843-1e3c-4d55-e652-8f8ee71d8b76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fbfccc91183169842aac8d093379f0a449b5a26c5ee7a298baf0d556f1499b90  alpaca-combined/consolidated.00.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 量化模型\n",
        "接下来我们使用[llama.cpp](https://github.com/ggerganov/llama.cpp)工具对上一步生成的全量版本权重进行转换，生成4-bit量化模型。\n",
        "\n",
        "### 编译工具\n",
        "\n",
        "首先对llama.cpp工具进行编译。"
      ],
      "metadata": {
        "id": "ueexcKo-Q_EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GbjsT2wRRCR",
        "outputId": "4b3925a2-436b-4992-887d-2d143b07a022",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c llama.cpp -o llama.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/common.cpp -o common.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/sampling.cpp -o sampling.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/build-info.cpp -o build-info.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/json-schema-to-grammar.cpp -o json-schema-to-grammar.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/console.cpp -o console.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c sgemm.cpp -o sgemm.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c unicode.cpp -o unicode.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c unicode-data.cpp -o unicode-data.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/main/main.cpp -o examples/main/main.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/main/main.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize/quantize.o -o quantize  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize-stats/quantize-stats.o -o quantize-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/perplexity/perplexity.o -o perplexity  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/imatrix/imatrix.o -o imatrix  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/embedding/embedding.o -o embedding  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/vdot.o -o vdot  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/q8dot.o -o q8dot  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/train.cpp -o train.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/simple/simple.o -o simple  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched/batched.o -o batched  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o common.o sampling.o grammar-parser.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched-bench/batched-bench.o -o batched-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/save-load-state/save-load-state.o -o save-load-state  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/server/server.cpp -o examples/server/server.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o -Iexamples/server examples/server/server.o -o server   \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf/gguf.o -o gguf  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf-split/gguf-split.o -o gguf-split  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/eval-callback/eval-callback.o -o eval-callback  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llama-bench/llama-bench.o -o llama-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/baby-llama/baby-llama.o -o baby-llama  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/beam-search/beam-search.o -o beam-search  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/retrieval/retrieval.o -o retrieval  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/speculative/speculative.o -o speculative  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/infill/infill.o -o infill  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/tokenize/tokenize.o -o tokenize  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/parallel/parallel.o -o parallel  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/finetune/finetune.o -o finetune  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/export-lora/export-lora.o -o export-lora  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookahead/lookahead.o -o lookahead  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/ngram-cache.cpp -o ngram-cache.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup.o -o lookup  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-create.o -o lookup-create  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-merge.o -o lookup-merge  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-stats.o -o lookup-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/passkey/passkey.o -o passkey  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gritlm/gritlm.o -o gritlm  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python convert.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCtkxUIarzO1",
        "outputId": "69ed2ca2-1e6a-4447-b976-6cc950944952"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: convert.py [-h] [--dump] [--dump-single] [--vocab-only] [--no-vocab]\n",
            "                  [--outtype {f32,f16,q8_0}] [--vocab-dir VOCAB_DIR] [--vocab-type VOCAB_TYPE]\n",
            "                  [--outfile OUTFILE] [--ctx CTX] [--concurrency CONCURRENCY] [--big-endian]\n",
            "                  [--pad-vocab] [--skip-unknown] [--verbose] [--metadata METADATA] [--get-outfile]\n",
            "                  model\n",
            "\n",
            "Convert a LLaMA model to a GGML compatible file\n",
            "\n",
            "positional arguments:\n",
            "  model                 directory containing model file, or model file itself (*.pth, *.pt, *.bin)\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --dump                don't convert, just show what's in the model\n",
            "  --dump-single         don't convert, just show what's in a single model file\n",
            "  --vocab-only          extract only the vocab\n",
            "  --no-vocab            store model without the vocab\n",
            "  --outtype {f32,f16,q8_0}\n",
            "                        output format - note: q8_0 may be very slow (default: f16 or f32 based on\n",
            "                        input)\n",
            "  --vocab-dir VOCAB_DIR\n",
            "                        directory containing tokenizer.model, if separate from model file\n",
            "  --vocab-type VOCAB_TYPE\n",
            "                        vocab types to try in order, choose from 'spm', 'bpe', 'hfft' (default:\n",
            "                        spm,hfft)\n",
            "  --outfile OUTFILE     path to write to; default: based on input\n",
            "  --ctx CTX             model training context (default: based on input)\n",
            "  --concurrency CONCURRENCY\n",
            "                        concurrency used for conversion (default: 8)\n",
            "  --big-endian          model is executed on big endian machine\n",
            "  --pad-vocab           add pad tokens when model vocab expects more than tokenizer metadata\n",
            "                        provides\n",
            "  --skip-unknown        skip unknown tensor names instead of failing\n",
            "  --verbose             increase output verbosity\n",
            "  --metadata METADATA   Specify the path for a metadata file\n",
            "  --get-outfile         get calculated default outfile name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型转换为ggml格式（FP16）\n",
        "\n",
        "这一步，我们将模型转换为ggml格式（FP16）。\n",
        "- 在这之前需要把`alpaca-combined`目录挪个位置，把模型文件放到`llama.cpp/zh-models/7B`下，把`tokenizer.model`放到`llama.cpp/zh-models`\n",
        "- tokenizer在哪里？\n",
        "    - `alpaca-combined`目录下有\n",
        "    - 或者从以下网址下载：https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b/resolve/main/tokenizer.model （注意，Alpaca和LLaMA的`tokenizer.model`不能混用！）\n",
        "\n",
        "💡 转换13B/33B模型提示：\n",
        "- tokenizer可以直接用7B的，13B/33B和7B的相同\n",
        "- Alpaca和LLaMA的`tokenizer.model`不能混用！\n",
        "- 以下看到7B字样的都是文件夹名，与转换过程没有关系了，改不改都行"
      ],
      "metadata": {
        "id": "gw2xpYC0RcQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv alpaca-combined llama-7b-alpaca-light\n",
        "!ls llama-7b-alpaca-light"
      ],
      "metadata": {
        "id": "5KgnFVStRjio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20381e8c-41e0-4689-ed1b-0399c360e2f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "consolidated.00.pth  params.json  special_tokens_map.json  tokenizer_config.json  tokenizer.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models\n",
        "!python llama.cpp/convert.py --outtype f16 --outfile models/llama-7b-alpaca-light-f16.gguf llama-7b-alpaca-light/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUHeoTMQS1AQ",
        "outputId": "9ad3754a-7895-4663-d56f-22f54ef5ce9c",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:convert:Loading model file llama-7b-alpaca-light/consolidated.00.pth\n",
            "INFO:convert:model parameters count : 6885494784 (7B)\n",
            "INFO:convert:params = Params(n_vocab=49954, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('llama-7b-alpaca-light'))\n",
            "INFO:convert:Loaded vocab file PosixPath('llama-7b-alpaca-light/tokenizer.model'), type 'spm'\n",
            "INFO:convert:Vocab info: <SentencePieceVocab with 49954 base tokens and 0 added tokens>\n",
            "INFO:convert:Special vocab info: <SpecialVocab with 0 merges, special tokens unset, add special tokens {'bos': True, 'eos': False}>\n",
            "INFO:convert:Writing models/llama-7b-alpaca-light-f16.gguf, format 1\n",
            "WARNING:convert:Ignoring added_tokens.json since model matches vocab size without it.\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:convert:[  1/291] Writing tensor token_embd.weight                      | size  49954 x   4096  | type F16  | T+   0\n",
            "INFO:convert:[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[  7/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
            "INFO:convert:[  8/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
            "INFO:convert:[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
            "INFO:convert:[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[ 16/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1\n",
            "INFO:convert:[ 17/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1\n",
            "INFO:convert:[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
            "INFO:convert:[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
            "INFO:convert:[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
            "INFO:convert:[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
            "INFO:convert:[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   7\n",
            "INFO:convert:[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   7\n",
            "INFO:convert:[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   7\n",
            "INFO:convert:[ 25/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   7\n",
            "INFO:convert:[ 26/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   8\n",
            "INFO:convert:[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   8\n",
            "INFO:convert:[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   8\n",
            "INFO:convert:[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   8\n",
            "INFO:convert:[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   8\n",
            "INFO:convert:[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   8\n",
            "INFO:convert:[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   8\n",
            "INFO:convert:[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   8\n",
            "INFO:convert:[ 34/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   8\n",
            "INFO:convert:[ 35/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   9\n",
            "INFO:convert:[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   9\n",
            "INFO:convert:[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   9\n",
            "INFO:convert:[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   9\n",
            "INFO:convert:[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   9\n",
            "INFO:convert:[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   9\n",
            "INFO:convert:[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   9\n",
            "INFO:convert:[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   9\n",
            "INFO:convert:[ 43/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  10\n",
            "INFO:convert:[ 44/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  10\n",
            "INFO:convert:[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  10\n",
            "INFO:convert:[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  10\n",
            "INFO:convert:[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  10\n",
            "INFO:convert:[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  10\n",
            "INFO:convert:[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  10\n",
            "INFO:convert:[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  10\n",
            "INFO:convert:[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  10\n",
            "INFO:convert:[ 52/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  11\n",
            "INFO:convert:[ 53/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  11\n",
            "INFO:convert:[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  11\n",
            "INFO:convert:[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  11\n",
            "INFO:convert:[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  11\n",
            "INFO:convert:[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  11\n",
            "INFO:convert:[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  11\n",
            "INFO:convert:[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  11\n",
            "INFO:convert:[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  11\n",
            "INFO:convert:[ 61/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  12\n",
            "INFO:convert:[ 62/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  12\n",
            "INFO:convert:[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  12\n",
            "INFO:convert:[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  12\n",
            "INFO:convert:[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  12\n",
            "INFO:convert:[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  12\n",
            "INFO:convert:[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  12\n",
            "INFO:convert:[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  12\n",
            "INFO:convert:[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  13\n",
            "INFO:convert:[ 70/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  13\n",
            "INFO:convert:[ 71/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  13\n",
            "INFO:convert:[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  13\n",
            "INFO:convert:[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  13\n",
            "INFO:convert:[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  13\n",
            "INFO:convert:[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  13\n",
            "INFO:convert:[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  13\n",
            "INFO:convert:[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  14\n",
            "INFO:convert:[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  14\n",
            "INFO:convert:[ 79/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  14\n",
            "INFO:convert:[ 80/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  14\n",
            "INFO:convert:[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  14\n",
            "INFO:convert:[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  14\n",
            "INFO:convert:[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  14\n",
            "INFO:convert:[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  14\n",
            "INFO:convert:[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  15\n",
            "INFO:convert:[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  15\n",
            "INFO:convert:[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  15\n",
            "INFO:convert:[ 88/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  15\n",
            "INFO:convert:[ 89/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  15\n",
            "INFO:convert:[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  15\n",
            "INFO:convert:[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  15\n",
            "INFO:convert:[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
            "INFO:convert:[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  16\n",
            "INFO:convert:[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  16\n",
            "INFO:convert:[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  16\n",
            "INFO:convert:[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  16\n",
            "INFO:convert:[ 97/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  16\n",
            "INFO:convert:[ 98/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  16\n",
            "INFO:convert:[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  16\n",
            "INFO:convert:[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  16\n",
            "INFO:convert:[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  16\n",
            "INFO:convert:[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  17\n",
            "INFO:convert:[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  17\n",
            "INFO:convert:[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  17\n",
            "INFO:convert:[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  17\n",
            "INFO:convert:[106/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  17\n",
            "INFO:convert:[107/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  17\n",
            "INFO:convert:[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  18\n",
            "INFO:convert:[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  18\n",
            "INFO:convert:[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  18\n",
            "INFO:convert:[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  18\n",
            "INFO:convert:[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  18\n",
            "INFO:convert:[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  18\n",
            "INFO:convert:[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  18\n",
            "INFO:convert:[115/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  18\n",
            "INFO:convert:[116/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  18\n",
            "INFO:convert:[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  19\n",
            "INFO:convert:[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  19\n",
            "INFO:convert:[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  19\n",
            "INFO:convert:[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  19\n",
            "INFO:convert:[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  19\n",
            "INFO:convert:[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  20\n",
            "INFO:convert:[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  20\n",
            "INFO:convert:[124/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  22\n",
            "INFO:convert:[125/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  23\n",
            "INFO:convert:[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  23\n",
            "INFO:convert:[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  23\n",
            "INFO:convert:[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  23\n",
            "INFO:convert:[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  23\n",
            "INFO:convert:[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  23\n",
            "INFO:convert:[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  23\n",
            "INFO:convert:[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  24\n",
            "INFO:convert:[133/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  24\n",
            "INFO:convert:[134/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  24\n",
            "INFO:convert:[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  24\n",
            "INFO:convert:[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  24\n",
            "INFO:convert:[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  24\n",
            "INFO:convert:[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  24\n",
            "INFO:convert:[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  24\n",
            "INFO:convert:[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  24\n",
            "INFO:convert:[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  24\n",
            "INFO:convert:[142/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  25\n",
            "INFO:convert:[143/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  25\n",
            "INFO:convert:[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  25\n",
            "INFO:convert:[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  25\n",
            "INFO:convert:[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  25\n",
            "INFO:convert:[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  25\n",
            "INFO:convert:[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  25\n",
            "INFO:convert:[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  26\n",
            "INFO:convert:[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  26\n",
            "INFO:convert:[151/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  26\n",
            "INFO:convert:[152/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  26\n",
            "INFO:convert:[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  26\n",
            "INFO:convert:[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  26\n",
            "INFO:convert:[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  26\n",
            "INFO:convert:[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  26\n",
            "INFO:convert:[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  27\n",
            "INFO:convert:[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  27\n",
            "INFO:convert:[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  27\n",
            "INFO:convert:[160/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  27\n",
            "INFO:convert:[161/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  27\n",
            "INFO:convert:[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  27\n",
            "INFO:convert:[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  27\n",
            "INFO:convert:[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  27\n",
            "INFO:convert:[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  28\n",
            "INFO:convert:[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  28\n",
            "INFO:convert:[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  28\n",
            "INFO:convert:[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  28\n",
            "INFO:convert:[169/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  28\n",
            "INFO:convert:[170/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  28\n",
            "INFO:convert:[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  29\n",
            "INFO:convert:[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  29\n",
            "INFO:convert:[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  29\n",
            "INFO:convert:[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  29\n",
            "INFO:convert:[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  29\n",
            "INFO:convert:[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  29\n",
            "INFO:convert:[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  29\n",
            "INFO:convert:[178/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  29\n",
            "INFO:convert:[179/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  29\n",
            "INFO:convert:[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  30\n",
            "INFO:convert:[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  30\n",
            "INFO:convert:[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  30\n",
            "INFO:convert:[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  30\n",
            "INFO:convert:[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  30\n",
            "INFO:convert:[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  30\n",
            "INFO:convert:[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  30\n",
            "INFO:convert:[187/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  30\n",
            "INFO:convert:[188/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  30\n",
            "INFO:convert:[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  31\n",
            "INFO:convert:[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  31\n",
            "INFO:convert:[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  31\n",
            "INFO:convert:[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  31\n",
            "INFO:convert:[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  31\n",
            "INFO:convert:[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  31\n",
            "INFO:convert:[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  31\n",
            "INFO:convert:[196/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  31\n",
            "INFO:convert:[197/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  32\n",
            "INFO:convert:[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  32\n",
            "INFO:convert:[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  32\n",
            "INFO:convert:[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  32\n",
            "INFO:convert:[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  32\n",
            "INFO:convert:[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  32\n",
            "INFO:convert:[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  32\n",
            "INFO:convert:[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  32\n",
            "INFO:convert:[205/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  32\n",
            "INFO:convert:[206/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  33\n",
            "INFO:convert:[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  33\n",
            "INFO:convert:[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  33\n",
            "INFO:convert:[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  33\n",
            "INFO:convert:[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  33\n",
            "INFO:convert:[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  33\n",
            "INFO:convert:[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  33\n",
            "INFO:convert:[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  33\n",
            "INFO:convert:[214/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  33\n",
            "INFO:convert:[215/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  34\n",
            "INFO:convert:[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  34\n",
            "INFO:convert:[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[223/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  35\n",
            "INFO:convert:[224/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  35\n",
            "INFO:convert:[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  35\n",
            "INFO:convert:[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  35\n",
            "INFO:convert:[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  35\n",
            "INFO:convert:[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  35\n",
            "INFO:convert:[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  35\n",
            "INFO:convert:[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  35\n",
            "INFO:convert:[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  35\n",
            "INFO:convert:[232/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  36\n",
            "INFO:convert:[233/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  41\n",
            "INFO:convert:[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  41\n",
            "INFO:convert:[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[241/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  41\n",
            "INFO:convert:[242/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  41\n",
            "INFO:convert:[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  41\n",
            "INFO:convert:[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  41\n",
            "INFO:convert:[250/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  42\n",
            "INFO:convert:[251/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  42\n",
            "INFO:convert:[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  42\n",
            "INFO:convert:[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  42\n",
            "INFO:convert:[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  42\n",
            "INFO:convert:[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  42\n",
            "INFO:convert:[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  42\n",
            "INFO:convert:[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  42\n",
            "INFO:convert:[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  43\n",
            "INFO:convert:[259/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  43\n",
            "INFO:convert:[260/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  43\n",
            "INFO:convert:[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  43\n",
            "INFO:convert:[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  43\n",
            "INFO:convert:[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  43\n",
            "INFO:convert:[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  43\n",
            "INFO:convert:[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  44\n",
            "INFO:convert:[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  44\n",
            "INFO:convert:[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  44\n",
            "INFO:convert:[268/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  44\n",
            "INFO:convert:[269/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  44\n",
            "INFO:convert:[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  44\n",
            "INFO:convert:[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  44\n",
            "INFO:convert:[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  44\n",
            "INFO:convert:[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  45\n",
            "INFO:convert:[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  45\n",
            "INFO:convert:[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  45\n",
            "INFO:convert:[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  45\n",
            "INFO:convert:[277/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  45\n",
            "INFO:convert:[278/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  45\n",
            "INFO:convert:[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  45\n",
            "INFO:convert:[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  45\n",
            "INFO:convert:[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  45\n",
            "INFO:convert:[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  46\n",
            "INFO:convert:[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  46\n",
            "INFO:convert:[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  46\n",
            "INFO:convert:[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  46\n",
            "INFO:convert:[286/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  46\n",
            "INFO:convert:[287/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  46\n",
            "INFO:convert:[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  47\n",
            "INFO:convert:[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  47\n",
            "INFO:convert:[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  47\n",
            "INFO:convert:[291/291] Writing tensor output.weight                          | size  49954 x   4096  | type F16  | T+  47\n",
            "INFO:convert:Wrote models/llama-7b-alpaca-light-f16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 将FP16模型量化为4-bit\n",
        "\n",
        "我们进一步将FP16模型转换为4-bit量化模型，此处选择的是新版Q4_K方法。"
      ],
      "metadata": {
        "id": "hEZEJAVYCHkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./quantize ../models/llama-7b-alpaca-light-f16.gguf ../models/llama-7b-alpaca-light-Q4_K_M.gguf Q4_K_M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xyais7OUVDI",
        "outputId": "c2763ba9-acbb-4b0f-d8be-8473ccca0927",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 2947 (26cd4237)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '../models/llama-7b-alpaca-light-f16.gguf' to '../models/llama-7b-alpaca-light-Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 18 key-value pairs and 291 tensors from ../models/llama-7b-alpaca-light-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = llama-7b-alpaca-light\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 49954\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,49954]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,49954]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,49954]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  17:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 49954,     1,     1], type =    f16, converting to q4_K .. size =   390.27 MiB ->   109.76 MiB\n",
            "[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                        output.weight - [ 4096, 49954,     1,     1], type =    f16, converting to q6_K .. size =   390.27 MiB ->   160.07 MiB\n",
            "llama_model_quantize_internal: model size  = 13133.55 MB\n",
            "llama_model_quantize_internal: quant size  =  3988.22 MB\n",
            "\n",
            "main: quantize time = 94493.79 ms\n",
            "main:    total time = 94493.79 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### （可选）测试量化模型解码\n",
        "至此已完成了所有转换步骤。\n",
        "我们运行一条命令测试一下是否能够正常加载并进行对话。\n",
        "\n",
        "FP16和Q4量化文件存放在./llama.cpp/zh-models/7B下，可按需下载使用。"
      ],
      "metadata": {
        "id": "DLkuRAo9Vkb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./main -m ../models/llama-7b-alpaca-light-Q4_K_M.gguf --color -p \"详细介绍一下北京的名胜古迹：\" -n 1280"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW-ep1BsVQtG",
        "outputId": "eef2a4af-42b9-4f5b-ad6f-124d1f0ebd80",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2947 (26cd4237)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716204254\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-7b-alpaca-light-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = llama-7b-alpaca-light\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 49954\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,49954]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,49954]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,49954]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  17:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 581/49954 vs 259/49954 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 49954\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.89 B\n",
            "llm_load_print_meta: model size       = 3.89 GiB (4.86 BPW) \n",
            "llm_load_print_meta: general.name     = llama-7b-alpaca-light\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  3988.22 MiB\n",
            "................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   105.57 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 8 / 16 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 1280, n_keep = 1\n",
            "\n",
            "\n",
            "\u001b[33m<s> 详细介绍一下北京的名胜古迹：\u001b[0m长城、颐和园、天安门广场、故\n",
            "\n",
            "llama_print_timings:        load time =     523.50 ms\n",
            "llama_print_timings:      sample time =       0.79 ms /    12 runs   (    0.07 ms per token, 15132.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     582.55 ms /    11 tokens (   52.96 ms per token,    18.88 tokens per second)\n",
            "llama_print_timings:        eval time =    1023.63 ms /    11 runs   (   93.06 ms per token,    10.75 tokens per second)\n",
            "\n",
            "llama_print_timings:       total time =    1705.55 ms /    22 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 上传"
      ],
      "metadata": {
        "id": "6-UXzvPzv5L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U \"huggingface_hub[cli]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7kOuYsXvuN2",
        "outputId": "e5a33759-97ef-4de8-d387-a8d1fa1ae1e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Collecting huggingface_hub[cli]\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.11.0)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2024.2.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
            "Installing collected packages: pfzy, InquirerPy, huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed InquirerPy-0.3.4 huggingface_hub-0.23.0 pfzy-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1jONyi3v2y1",
        "outputId": "a33e4824-1c30-4428-f4b1-3126d3c391c6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上传原始模型"
      ],
      "metadata": {
        "id": "bB8dMzd7wbzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cd llama-7b-alpaca-light/ && huggingface-cli upload LightXXXXX/llama-7b-alpaca-light . ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yje6jyj4wALL",
        "outputId": "f2713f41-8dc6-4ca3-abc2-b4dbe75c4a0b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster uploads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "consolidated.00.pth:   0% 0.00/13.8G [00:00<?, ?B/s]\n",
            "tokenizer.model:   0% 0.00/758k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "consolidated.00.pth:   0% 16.4k/13.8G [00:00<30:13:04, 127kB/s]\n",
            "tokenizer.model: 100% 758k/758k [00:00<00:00, 1.26MB/s]\n",
            "consolidated.00.pth: 100% 13.8G/13.8G [05:12<00:00, 44.0MB/s]\n",
            "\n",
            "\n",
            "Upload 2 LFS files: 100% 2/2 [05:13<00:00, 156.72s/it]\n",
            "https://huggingface.co/LightXXXXX/llama-7b-alpaca-light/tree/main/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上传gguf"
      ],
      "metadata": {
        "id": "-QgrayT6weqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cd models/ && huggingface-cli upload LightXXXXX/llama-7b-alpaca-light-gguf . ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DYEsE-ewbH4",
        "outputId": "5023a2fd-57eb-4159-9920-2994b8c78c9d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster uploads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   0% 0.00/4.18G [00:00<?, ?B/s]\n",
            "Upload 2 LFS files:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   0% 0.00/13.8G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   0% 3.46M/4.18G [00:00<04:05, 17.0MB/s] \n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   0% 6.95M/4.18G [00:00<03:27, 20.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   0% 10.6M/4.18G [00:00<02:52, 24.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   1% 44.0M/4.18G [00:01<02:18, 29.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   1% 48.0M/4.18G [00:02<03:54, 17.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   1% 54.8M/4.18G [00:02<02:51, 24.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   1% 59.6M/4.18G [00:02<02:41, 25.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   2% 64.0M/4.18G [00:02<03:41, 18.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   2% 73.0M/4.18G [00:03<02:25, 28.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   2% 87.5M/4.18G [00:03<01:59, 34.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   2% 92.1M/4.18G [00:03<02:11, 31.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   0% 59.9M/13.8G [00:03<08:04, 28.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   2% 96.0M/4.18G [00:04<03:31, 19.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   2% 103M/4.18G [00:04<02:40, 25.4MB/s] \n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   3% 108M/4.18G [00:04<02:43, 25.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   1% 81.2M/13.8G [00:04<10:27, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   3% 124M/4.18G [00:04<02:02, 33.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   3% 136M/4.18G [00:05<02:00, 33.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   3% 141M/4.18G [00:05<02:04, 32.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   1% 112M/13.8G [00:05<10:14, 22.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   4% 158M/4.18G [00:05<01:54, 35.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   4% 164M/4.18G [00:06<02:17, 29.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   4% 168M/4.18G [00:06<02:07, 31.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   4% 188M/4.18G [00:07<02:34, 25.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   5% 192M/4.18G [00:07<03:36, 18.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   5% 199M/4.18G [00:07<02:37, 25.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   5% 208M/4.18G [00:08<02:19, 28.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   5% 222M/4.18G [00:08<01:38, 40.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   5% 227M/4.18G [00:08<01:48, 36.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   6% 233M/4.18G [00:08<01:35, 41.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   6% 240M/4.18G [00:08<02:03, 31.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   6% 253M/4.18G [00:08<01:21, 48.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   6% 270M/4.18G [00:09<01:13, 53.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   2% 240M/13.8G [00:09<06:26, 35.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   7% 283M/4.18G [00:09<01:39, 39.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   2% 259M/13.8G [00:09<05:53, 38.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   2% 267M/13.8G [00:09<05:05, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   7% 288M/4.18G [00:10<02:33, 25.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   7% 296M/4.18G [00:10<02:03, 31.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   8% 317M/4.18G [00:10<01:23, 46.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   2% 304M/13.8G [00:10<05:19, 42.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   8% 334M/4.18G [00:11<01:16, 50.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   8% 341M/4.18G [00:11<01:35, 40.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   2% 336M/13.8G [00:11<05:19, 42.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   9% 365M/4.18G [00:11<01:14, 51.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   9% 372M/4.18G [00:11<01:23, 45.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   3% 368M/13.8G [00:12<05:19, 42.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:   9% 392M/4.18G [00:12<01:25, 44.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  10% 398M/4.18G [00:12<01:36, 39.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   3% 400M/13.8G [00:12<05:29, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  10% 402M/4.18G [00:13<02:26, 25.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  10% 407M/4.18G [00:13<02:11, 28.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  10% 414M/4.18G [00:13<01:49, 34.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   3% 435M/13.8G [00:13<05:06, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  10% 418M/4.18G [00:13<02:41, 23.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  10% 427M/4.18G [00:13<01:57, 31.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  10% 432M/4.18G [00:14<02:12, 28.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  11% 444M/4.18G [00:14<01:31, 40.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  11% 449M/4.18G [00:14<01:47, 34.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  11% 462M/4.18G [00:14<01:16, 48.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  11% 468M/4.18G [00:14<01:32, 40.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  11% 478M/4.18G [00:14<01:16, 48.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  12% 484M/4.18G [00:15<01:31, 40.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   4% 512M/13.8G [00:15<06:28, 34.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  12% 510M/4.18G [00:15<01:09, 52.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  12% 517M/4.18G [00:15<01:24, 43.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   4% 544M/13.8G [00:15<05:27, 40.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  13% 542M/4.18G [00:16<01:04, 56.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  15% 644M/4.18G [00:18<01:15, 46.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  16% 656M/4.18G [00:18<01:22, 42.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  16% 668M/4.18G [00:19<01:05, 53.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  16% 686M/4.18G [00:19<01:05, 53.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   4% 592M/13.8G [00:19<13:53, 15.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  17% 693M/4.18G [00:19<01:16, 45.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   4% 613M/13.8G [00:19<07:53, 27.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  17% 718M/4.18G [00:20<01:03, 54.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   5% 630M/13.8G [00:20<06:09, 35.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  18% 736M/4.18G [00:20<01:17, 44.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  18% 749M/4.18G [00:20<01:00, 57.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  18% 757M/4.18G [00:20<01:12, 47.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   5% 656M/13.8G [00:21<07:35, 28.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  19% 781M/4.18G [00:21<00:59, 57.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   5% 674M/13.8G [00:21<05:34, 39.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  19% 789M/4.18G [00:21<01:09, 48.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   5% 690M/13.8G [00:21<04:59, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  19% 800M/4.18G [00:21<01:17, 43.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  19% 813M/4.18G [00:21<01:00, 56.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  20% 821M/4.18G [00:22<01:12, 46.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   5% 720M/13.8G [00:22<05:24, 40.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  20% 845M/4.18G [00:22<00:59, 56.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   5% 737M/13.8G [00:22<05:10, 42.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  20% 853M/4.18G [00:22<01:16, 43.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  21% 863M/4.18G [00:23<01:05, 50.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  21% 869M/4.18G [00:23<01:23, 39.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  21% 877M/4.18G [00:23<01:13, 45.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  21% 893M/4.18G [00:23<01:04, 50.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   6% 784M/13.8G [00:23<06:14, 34.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  22% 910M/4.18G [00:24<00:58, 56.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  22% 917M/4.18G [00:24<01:08, 47.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   6% 816M/13.8G [00:24<04:54, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  22% 941M/4.18G [00:24<00:58, 55.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  23% 948M/4.18G [00:24<01:05, 49.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   6% 848M/13.8G [00:25<04:57, 43.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  23% 973M/4.18G [00:25<00:58, 55.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  23% 980M/4.18G [00:25<01:08, 46.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   6% 880M/13.8G [00:25<04:52, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  24% 1.01G/4.18G [00:26<00:54, 58.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   7% 898M/13.8G [00:26<04:52, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  24% 1.01G/4.18G [00:26<01:08, 46.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   7% 918M/13.8G [00:26<04:22, 48.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  25% 1.04G/4.18G [00:26<00:54, 57.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  25% 1.04G/4.18G [00:26<01:11, 44.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   7% 944M/13.8G [00:27<05:51, 36.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  26% 1.07G/4.18G [00:27<01:04, 48.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  26% 1.08G/4.18G [00:27<01:17, 40.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   7% 976M/13.8G [00:27<05:24, 39.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  26% 1.10G/4.18G [00:28<00:57, 53.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   7% 995M/13.8G [00:28<05:04, 41.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  26% 1.11G/4.18G [00:28<01:07, 45.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  27% 1.13G/4.18G [00:28<00:54, 56.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   7% 1.02G/13.8G [00:28<04:57, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  27% 1.14G/4.18G [00:29<01:01, 49.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  28% 1.15G/4.18G [00:29<01:05, 46.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  28% 1.16G/4.18G [00:29<01:00, 50.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   8% 1.06G/13.8G [00:29<04:54, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  28% 1.18G/4.18G [00:29<00:54, 55.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  28% 1.19G/4.18G [00:30<01:04, 46.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  29% 1.19G/4.18G [00:30<01:02, 47.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  29% 1.20G/4.18G [00:30<01:11, 41.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  29% 1.22G/4.18G [00:30<00:54, 54.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  29% 1.22G/4.18G [00:30<01:18, 37.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  29% 1.23G/4.18G [00:31<01:05, 45.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   8% 1.13G/13.8G [00:31<04:14, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  30% 1.24G/4.18G [00:31<01:27, 33.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  30% 1.24G/4.18G [00:31<01:18, 37.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  30% 1.25G/4.18G [00:31<01:32, 31.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  30% 1.26G/4.18G [00:31<01:16, 38.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  30% 1.26G/4.18G [00:31<01:06, 44.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   8% 1.17G/13.8G [00:32<06:29, 32.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.18G/13.8G [00:32<04:06, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.19G/13.8G [00:32<04:42, 44.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.20G/13.8G [00:32<04:45, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.21G/13.8G [00:32<03:28, 60.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.22G/13.8G [00:33<04:16, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.23G/13.8G [00:33<04:19, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.25G/13.8G [00:33<03:16, 63.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.25G/13.8G [00:33<04:01, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.26G/13.8G [00:33<04:12, 49.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.28G/13.8G [00:33<03:13, 64.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.29G/13.8G [00:34<03:58, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:   9% 1.30G/13.8G [00:34<04:22, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  10% 1.31G/13.8G [00:34<03:19, 62.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  10% 1.32G/13.8G [00:34<04:01, 51.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  10% 1.33G/13.8G [00:35<04:51, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  10% 1.34G/13.8G [00:35<03:37, 57.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  30% 1.27G/4.18G [00:35<07:39, 6.34MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  10% 1.36G/13.8G [00:35<04:40, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  31% 1.28G/4.18G [00:36<05:25, 8.93MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  31% 1.29G/4.18G [00:36<03:19, 14.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  31% 1.30G/4.18G [00:36<03:12, 15.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  31% 1.31G/4.18G [00:36<02:12, 21.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  10% 1.40G/13.8G [00:36<04:26, 46.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  31% 1.31G/4.18G [00:36<02:12, 21.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  32% 1.32G/4.18G [00:36<01:42, 28.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  32% 1.33G/4.18G [00:37<01:28, 32.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  32% 1.33G/4.18G [00:37<01:35, 30.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  32% 1.34G/4.18G [00:37<01:14, 38.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  10% 1.44G/13.8G [00:37<05:47, 35.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  32% 1.35G/4.18G [00:37<01:15, 37.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  11% 1.46G/13.8G [00:37<04:12, 48.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  33% 1.37G/4.18G [00:38<00:56, 49.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  33% 1.39G/4.18G [00:38<00:52, 53.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  11% 1.49G/13.8G [00:38<04:41, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  34% 1.41G/4.18G [00:38<00:50, 55.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  11% 1.51G/13.8G [00:38<04:43, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  34% 1.41G/4.18G [00:39<01:01, 45.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  34% 1.44G/4.18G [00:39<00:49, 55.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  11% 1.54G/13.8G [00:39<04:52, 41.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  35% 1.45G/4.18G [00:39<01:03, 43.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  35% 1.45G/4.18G [00:39<00:54, 49.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  35% 1.46G/4.18G [00:40<01:14, 36.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  35% 1.47G/4.18G [00:40<01:05, 41.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  11% 1.58G/13.8G [00:40<04:52, 41.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  12% 1.58G/13.8G [00:40<05:36, 36.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  35% 1.47G/4.18G [00:40<01:55, 23.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  35% 1.48G/4.18G [00:41<01:42, 26.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  36% 1.49G/4.18G [00:41<01:27, 31.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  36% 1.49G/4.18G [00:41<01:35, 28.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  36% 1.50G/4.18G [00:41<01:25, 31.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  36% 1.52G/4.18G [00:41<00:59, 44.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  12% 1.65G/13.8G [00:42<05:21, 37.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  37% 1.54G/4.18G [00:42<00:53, 49.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  37% 1.54G/4.18G [00:42<01:02, 42.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  12% 1.68G/13.8G [00:42<04:45, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  37% 1.56G/4.18G [00:42<00:48, 53.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  38% 1.58G/4.18G [00:43<00:50, 51.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  38% 1.59G/4.18G [00:43<00:57, 45.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  38% 1.59G/4.18G [00:43<00:55, 46.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  13% 1.73G/13.8G [00:43<04:40, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  39% 1.61G/4.18G [00:44<00:52, 48.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  13% 1.75G/13.8G [00:44<04:26, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  13% 1.76G/13.8G [00:44<04:33, 43.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  39% 1.62G/4.18G [00:44<01:42, 25.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  39% 1.63G/4.18G [00:44<01:25, 29.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  39% 1.63G/4.18G [00:45<01:26, 29.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  39% 1.64G/4.18G [00:45<01:01, 41.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  40% 1.66G/4.18G [00:45<00:53, 47.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  13% 1.81G/13.8G [00:45<05:47, 34.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  40% 1.67G/4.18G [00:45<01:08, 36.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  40% 1.68G/4.18G [00:45<00:50, 49.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  40% 1.69G/4.18G [00:46<01:01, 40.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  41% 1.70G/4.18G [00:46<00:52, 47.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  41% 1.70G/4.18G [00:46<01:00, 41.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  13% 1.86G/13.8G [00:46<05:18, 37.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  41% 1.73G/4.18G [00:46<00:44, 55.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  14% 1.87G/13.8G [00:47<05:05, 38.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  42% 1.74G/4.18G [00:47<00:46, 52.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  42% 1.75G/4.18G [00:47<00:53, 45.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  14% 1.90G/13.8G [00:47<04:49, 41.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  42% 1.77G/4.18G [00:47<00:45, 53.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  14% 1.93G/13.8G [00:47<04:06, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  14% 1.93G/13.8G [00:48<03:39, 54.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  43% 1.78G/4.18G [00:48<01:04, 37.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  43% 1.79G/4.18G [00:48<01:01, 38.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  43% 1.79G/4.18G [00:48<01:10, 33.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  43% 1.81G/4.18G [00:48<00:55, 43.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  43% 1.81G/4.18G [00:49<01:00, 39.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  44% 1.82G/4.18G [00:49<00:54, 43.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  14% 1.98G/13.8G [00:49<05:10, 37.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  44% 1.83G/4.18G [00:49<00:54, 42.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  44% 1.84G/4.18G [00:49<01:13, 31.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  44% 1.85G/4.18G [00:50<00:52, 44.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  15% 2.02G/13.8G [00:50<03:42, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  44% 1.86G/4.18G [00:50<01:04, 36.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  45% 1.87G/4.18G [00:50<00:48, 47.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  45% 1.89G/4.18G [00:50<00:41, 55.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  15% 2.05G/13.8G [00:50<05:16, 37.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  45% 1.89G/4.18G [00:50<00:47, 48.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  15% 2.07G/13.8G [00:51<04:20, 45.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  46% 1.90G/4.18G [00:51<01:06, 34.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  46% 1.91G/4.18G [00:51<00:57, 39.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  46% 1.92G/4.18G [00:51<01:05, 34.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  46% 1.93G/4.18G [00:51<00:47, 46.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  48% 1.99G/4.18G [00:53<00:46, 47.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  15% 2.13G/13.8G [00:53<11:20, 17.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  48% 2.01G/4.18G [00:53<00:41, 52.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  16% 2.14G/13.8G [00:53<08:23, 23.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  49% 2.03G/4.18G [00:54<00:39, 54.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  16% 2.16G/13.8G [00:54<06:44, 28.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  49% 2.04G/4.18G [00:54<00:51, 41.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  49% 2.05G/4.18G [00:54<00:45, 46.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  49% 2.05G/4.18G [00:54<00:51, 41.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  49% 2.06G/4.18G [00:54<00:46, 46.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  16% 2.20G/13.8G [00:54<04:09, 46.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  49% 2.06G/4.18G [00:55<00:55, 38.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  50% 2.08G/4.18G [00:55<00:42, 49.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  50% 2.08G/4.18G [00:55<00:55, 37.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  50% 2.09G/4.18G [00:55<00:44, 46.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  16% 2.23G/13.8G [00:55<04:20, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  50% 2.10G/4.18G [00:55<00:56, 37.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  50% 2.10G/4.18G [00:55<00:46, 44.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  51% 2.12G/4.18G [00:56<00:49, 41.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  51% 2.12G/4.18G [00:56<00:44, 46.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  16% 2.26G/13.8G [00:56<04:26, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  51% 2.13G/4.18G [00:56<00:59, 34.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  51% 2.14G/4.18G [00:56<00:52, 38.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  51% 2.15G/4.18G [00:56<00:50, 39.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  52% 2.15G/4.18G [00:57<00:42, 47.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  17% 2.30G/13.8G [00:57<04:34, 41.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  52% 2.16G/4.18G [00:57<00:56, 36.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  52% 2.17G/4.18G [00:57<00:41, 48.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  52% 2.18G/4.18G [00:57<00:54, 37.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  52% 2.19G/4.18G [00:57<00:35, 55.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  53% 2.20G/4.18G [00:58<00:45, 44.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  17% 2.34G/13.8G [00:58<05:33, 34.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  53% 2.22G/4.18G [00:58<00:35, 55.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  54% 2.24G/4.18G [00:58<00:33, 57.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  17% 2.37G/13.8G [00:58<04:43, 40.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  54% 2.25G/4.18G [00:59<00:46, 41.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  54% 2.25G/4.18G [00:59<00:41, 46.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  54% 2.26G/4.18G [00:59<00:55, 34.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  54% 2.27G/4.18G [00:59<00:45, 42.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  54% 2.27G/4.18G [00:59<00:51, 37.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  55% 2.29G/4.18G [00:59<00:37, 50.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  55% 2.30G/4.18G [01:00<00:34, 55.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  18% 2.43G/13.8G [01:00<04:45, 39.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  55% 2.31G/4.18G [01:00<00:45, 41.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  18% 2.45G/13.8G [01:00<04:28, 42.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  55% 2.32G/4.18G [01:00<00:52, 35.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  56% 2.33G/4.18G [01:01<00:41, 45.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  56% 2.34G/4.18G [01:01<00:50, 36.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  18% 2.48G/13.8G [01:01<04:46, 39.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  56% 2.35G/4.18G [01:01<00:42, 43.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  56% 2.35G/4.18G [01:01<00:53, 34.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  56% 2.36G/4.18G [01:01<00:42, 42.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  57% 2.37G/4.18G [01:02<00:39, 45.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  57% 2.37G/4.18G [01:02<00:48, 37.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  57% 2.38G/4.18G [01:02<00:44, 40.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  57% 2.38G/4.18G [01:02<00:51, 34.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  57% 2.39G/4.18G [01:02<00:37, 47.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  18% 2.54G/13.8G [01:02<04:35, 40.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  18% 2.54G/13.8G [01:02<05:06, 36.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  19% 2.56G/13.8G [01:03<03:13, 58.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  58% 2.41G/4.18G [01:03<01:02, 28.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  19% 2.58G/13.8G [01:03<04:07, 45.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  58% 2.43G/4.18G [01:03<00:39, 44.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  58% 2.44G/4.18G [01:04<00:47, 37.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  58% 2.44G/4.18G [01:04<00:40, 43.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  59% 2.45G/4.18G [01:04<00:45, 38.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  59% 2.46G/4.18G [01:04<00:38, 45.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  19% 2.63G/13.8G [01:04<03:30, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  60% 2.49G/4.18G [01:05<00:31, 53.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  19% 2.65G/13.8G [01:05<06:28, 28.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  60% 2.50G/4.18G [01:05<00:37, 44.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  19% 2.66G/13.8G [01:05<05:31, 33.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  60% 2.53G/4.18G [01:05<00:27, 59.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  19% 2.67G/13.8G [01:05<05:36, 33.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  61% 2.53G/4.18G [01:06<00:35, 46.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  61% 2.54G/4.18G [01:06<00:33, 48.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  61% 2.55G/4.18G [01:06<00:40, 40.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  61% 2.56G/4.18G [01:06<00:32, 50.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  61% 2.56G/4.18G [01:06<00:40, 40.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  20% 2.72G/13.8G [01:06<05:16, 34.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  62% 2.59G/4.18G [01:07<00:27, 57.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  20% 2.74G/13.8G [01:07<04:21, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  62% 2.61G/4.18G [01:07<00:36, 42.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  63% 2.62G/4.18G [01:07<00:31, 50.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  63% 2.64G/4.18G [01:08<00:26, 57.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  20% 2.77G/13.8G [01:08<07:44, 23.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  63% 2.65G/4.18G [01:08<00:38, 40.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  64% 2.67G/4.18G [01:09<00:32, 46.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  20% 2.80G/13.8G [01:09<05:03, 36.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  64% 2.69G/4.18G [01:09<00:31, 48.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  20% 2.82G/13.8G [01:09<04:29, 40.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  64% 2.69G/4.18G [01:09<00:34, 42.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  65% 2.70G/4.18G [01:09<00:29, 49.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  65% 2.71G/4.18G [01:10<00:37, 39.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  65% 2.72G/4.18G [01:10<00:31, 46.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  65% 2.72G/4.18G [01:10<00:39, 37.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  65% 2.73G/4.18G [01:10<00:32, 44.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  21% 2.87G/13.8G [01:10<03:51, 47.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  65% 2.74G/4.18G [01:10<00:41, 34.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  66% 2.75G/4.18G [01:10<00:29, 48.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  21% 2.89G/13.8G [01:11<04:06, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  21% 2.90G/13.8G [01:11<04:54, 36.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  66% 2.76G/4.18G [01:11<00:48, 29.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  66% 2.77G/4.18G [01:11<00:38, 37.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  67% 2.78G/4.18G [01:11<00:31, 44.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  21% 2.93G/13.8G [01:11<04:35, 39.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  67% 2.79G/4.18G [01:12<00:39, 35.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  21% 2.95G/13.8G [01:12<04:49, 37.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  67% 2.81G/4.18G [01:12<00:27, 50.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  67% 2.82G/4.18G [01:12<00:32, 41.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  68% 2.82G/4.18G [01:12<00:31, 42.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  68% 2.85G/4.18G [01:13<00:26, 49.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  22% 2.99G/13.8G [01:13<05:10, 34.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  68% 2.86G/4.18G [01:13<00:28, 46.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  69% 2.88G/4.18G [01:14<00:25, 51.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  69% 2.88G/4.18G [01:14<00:28, 45.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  69% 2.90G/4.18G [01:14<00:26, 47.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  69% 2.90G/4.18G [01:14<00:31, 41.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  69% 2.91G/4.18G [01:14<00:29, 43.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  70% 2.91G/4.18G [01:15<00:36, 34.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  70% 2.92G/4.18G [01:15<00:30, 41.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  70% 2.93G/4.18G [01:15<00:37, 33.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  70% 2.94G/4.18G [01:15<00:25, 49.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  71% 2.96G/4.18G [01:15<00:21, 56.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  23% 3.10G/13.8G [01:16<04:19, 41.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  71% 2.97G/4.18G [01:16<00:27, 44.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  71% 2.98G/4.18G [01:16<00:28, 42.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  71% 2.98G/4.18G [01:16<00:25, 47.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  72% 3.00G/4.18G [01:16<00:24, 48.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  23% 3.15G/13.8G [01:17<04:17, 41.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  72% 3.02G/4.18G [01:17<00:22, 52.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  23% 3.17G/13.8G [01:17<03:48, 46.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  72% 3.03G/4.18G [01:17<00:27, 42.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  73% 3.04G/4.18G [01:17<00:22, 50.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  73% 3.04G/4.18G [01:17<00:26, 42.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  73% 3.05G/4.18G [01:18<00:22, 49.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  73% 3.07G/4.18G [01:18<00:20, 54.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  23% 3.22G/13.8G [01:18<05:08, 34.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  74% 3.09G/4.18G [01:18<00:20, 52.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  74% 3.10G/4.18G [01:19<00:25, 43.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  24% 3.25G/13.8G [01:19<04:30, 38.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  75% 3.12G/4.18G [01:19<00:19, 55.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  75% 3.12G/4.18G [01:19<00:24, 44.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  24% 3.28G/13.8G [01:19<03:55, 44.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  75% 3.15G/4.18G [01:20<00:19, 54.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  24% 3.30G/13.8G [01:20<04:09, 41.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  75% 3.16G/4.18G [01:20<00:22, 45.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  24% 3.32G/13.8G [01:20<04:02, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  76% 3.18G/4.18G [01:20<00:18, 55.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  24% 3.33G/13.8G [01:20<03:42, 47.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  76% 3.19G/4.18G [01:21<00:29, 34.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  76% 3.19G/4.18G [01:21<00:25, 38.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  77% 3.21G/4.18G [01:21<00:21, 44.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  25% 3.38G/13.8G [01:21<03:58, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  77% 3.22G/4.18G [01:22<00:29, 32.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  77% 3.23G/4.18G [01:22<00:22, 41.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  81% 3.37G/4.18G [01:25<00:13, 58.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  25% 3.41G/13.8G [01:25<20:35, 8.38MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  81% 3.38G/4.18G [01:25<00:16, 48.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  25% 3.42G/13.8G [01:25<13:01, 13.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  81% 3.39G/4.18G [01:25<00:18, 41.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  81% 3.40G/4.18G [01:25<00:15, 48.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  25% 3.45G/13.8G [01:25<06:18, 27.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  82% 3.42G/4.18G [01:26<00:13, 54.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  82% 3.43G/4.18G [01:26<00:18, 39.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  25% 3.47G/13.8G [01:26<06:03, 28.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  82% 3.44G/4.18G [01:26<00:21, 35.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  83% 3.45G/4.18G [01:27<00:16, 44.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  83% 3.46G/4.18G [01:27<00:18, 38.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  83% 3.47G/4.18G [01:27<00:15, 46.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  26% 3.51G/13.8G [01:27<03:55, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  26% 3.52G/13.8G [01:27<04:59, 34.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  26% 3.53G/13.8G [01:27<03:21, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  26% 3.54G/13.8G [01:28<06:16, 27.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  26% 3.55G/13.8G [01:28<05:27, 31.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  26% 3.57G/13.8G [01:28<03:50, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  83% 3.47G/4.18G [01:29<01:02, 11.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  26% 3.58G/13.8G [01:29<04:13, 40.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  83% 3.49G/4.18G [01:29<00:37, 18.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  84% 3.49G/4.18G [01:29<00:36, 18.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  84% 3.50G/4.18G [01:30<00:24, 27.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  84% 3.51G/4.18G [01:30<00:24, 27.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  26% 3.63G/13.8G [01:30<03:46, 44.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  84% 3.52G/4.18G [01:30<00:22, 29.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  84% 3.53G/4.18G [01:30<00:14, 43.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  85% 3.54G/4.18G [01:30<00:16, 38.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  27% 3.66G/13.8G [01:31<04:33, 36.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  85% 3.57G/4.18G [01:31<00:12, 48.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  27% 3.69G/13.8G [01:31<03:25, 49.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  85% 3.57G/4.18G [01:31<00:14, 41.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  27% 3.70G/13.8G [01:31<03:57, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  86% 3.60G/4.18G [01:32<00:11, 51.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  27% 3.71G/13.8G [01:32<04:06, 40.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  86% 3.61G/4.18G [01:32<00:12, 46.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  27% 3.73G/13.8G [01:32<03:48, 43.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  87% 3.63G/4.18G [01:32<00:09, 59.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  87% 3.64G/4.18G [01:32<00:11, 47.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  87% 3.65G/4.18G [01:33<00:09, 54.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  87% 3.65G/4.18G [01:33<00:11, 47.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  27% 3.78G/13.8G [01:33<03:48, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  88% 3.68G/4.18G [01:33<00:08, 56.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  28% 3.80G/13.8G [01:33<03:37, 45.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  88% 3.68G/4.18G [01:33<00:10, 45.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  89% 3.71G/4.18G [01:34<00:08, 57.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  28% 3.82G/13.8G [01:34<03:53, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  89% 3.72G/4.18G [01:34<00:09, 49.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  89% 3.74G/4.18G [01:34<00:07, 56.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  28% 3.86G/13.8G [01:35<04:08, 40.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  90% 3.75G/4.18G [01:35<00:09, 47.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  90% 3.77G/4.18G [01:35<00:07, 54.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  28% 3.89G/13.8G [01:35<03:48, 43.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  90% 3.78G/4.18G [01:35<00:08, 45.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  28% 3.91G/13.8G [01:36<03:37, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  91% 3.81G/4.18G [01:36<00:06, 55.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  91% 3.81G/4.18G [01:36<00:08, 44.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  29% 3.94G/13.8G [01:36<04:04, 40.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  91% 3.82G/4.18G [01:36<00:09, 38.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  92% 3.83G/4.18G [01:37<00:07, 45.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  92% 3.84G/4.18G [01:37<00:07, 48.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  29% 3.97G/13.8G [01:37<03:05, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  92% 3.85G/4.18G [01:37<00:08, 41.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  92% 3.85G/4.18G [01:37<00:07, 44.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  92% 3.86G/4.18G [01:37<00:09, 34.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  92% 3.86G/4.18G [01:37<00:08, 38.5MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  93% 3.88G/4.18G [01:38<00:06, 49.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  29% 4.02G/13.8G [01:38<03:35, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  93% 3.89G/4.18G [01:38<00:08, 35.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  93% 3.90G/4.18G [01:38<00:06, 44.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  93% 3.90G/4.18G [01:38<00:07, 36.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  94% 3.92G/4.18G [01:39<00:05, 49.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  94% 3.92G/4.18G [01:39<00:06, 41.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  94% 3.93G/4.18G [01:39<00:04, 51.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  30% 4.07G/13.8G [01:39<03:27, 46.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  94% 3.94G/4.18G [01:39<00:06, 37.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  94% 3.95G/4.18G [01:39<00:05, 40.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  94% 3.95G/4.18G [01:40<00:07, 32.7MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  95% 3.96G/4.18G [01:40<00:05, 42.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  95% 3.98G/4.18G [01:40<00:03, 52.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  30% 4.11G/13.8G [01:40<05:03, 31.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  95% 3.99G/4.18G [01:40<00:04, 43.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  96% 4.00G/4.18G [01:40<00:03, 54.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  30% 4.14G/13.8G [01:40<03:20, 48.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  96% 4.01G/4.18G [01:41<00:03, 44.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  30% 4.15G/13.8G [01:41<03:22, 47.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  96% 4.03G/4.18G [01:41<00:03, 49.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  30% 4.17G/13.8G [01:41<04:13, 37.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  96% 4.04G/4.18G [01:41<00:03, 38.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  97% 4.05G/4.18G [01:42<00:02, 46.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  97% 4.05G/4.18G [01:42<00:03, 38.0MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  97% 4.06G/4.18G [01:42<00:02, 48.9MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  97% 4.07G/4.18G [01:42<00:02, 41.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  31% 4.21G/13.8G [01:42<04:03, 39.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  31% 4.22G/13.8G [01:42<03:06, 51.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  98% 4.08G/4.18G [01:43<00:03, 33.3MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  98% 4.09G/4.18G [01:43<00:02, 43.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  31% 4.24G/13.8G [01:43<03:33, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  98% 4.10G/4.18G [01:43<00:02, 35.8MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  99% 4.12G/4.18G [01:44<00:01, 48.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  31% 4.27G/13.8G [01:44<03:46, 41.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  99% 4.14G/4.18G [01:44<00:00, 51.4MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  31% 4.29G/13.8G [01:44<03:08, 50.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf:  99% 4.15G/4.18G [01:44<00:00, 43.6MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  31% 4.31G/13.8G [01:44<03:20, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf: 100% 4.17G/4.18G [01:45<00:00, 58.1MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  31% 4.32G/13.8G [01:45<03:50, 41.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf: 100% 4.18G/4.18G [01:45<00:00, 47.2MB/s]\n",
            "\n",
            "llama-7b-alpaca-light-Q4_K_M.gguf: 100% 4.18G/4.18G [01:45<00:00, 39.6MB/s]\n",
            "\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.35G/13.8G [01:45<04:04, 38.6MB/s]\u001b[A\u001b[A\n",
            "Upload 2 LFS files:  50% 1/2 [01:45<01:45, 105.87s/it]\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.37G/13.8G [01:45<02:53, 54.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.37G/13.8G [01:46<03:26, 45.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.38G/13.8G [01:46<03:26, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.40G/13.8G [01:46<02:35, 60.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.41G/13.8G [01:46<03:00, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.42G/13.8G [01:46<03:10, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.43G/13.8G [01:47<02:26, 64.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.44G/13.8G [01:47<02:50, 54.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.45G/13.8G [01:47<03:13, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.46G/13.8G [01:47<02:27, 63.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  32% 4.47G/13.8G [01:47<02:53, 53.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.48G/13.8G [01:48<03:16, 47.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.49G/13.8G [01:48<02:30, 61.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.50G/13.8G [01:48<02:56, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.51G/13.8G [01:48<03:24, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.53G/13.8G [01:48<02:33, 60.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.53G/13.8G [01:49<03:22, 45.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.54G/13.8G [01:49<03:36, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.56G/13.8G [01:49<02:42, 56.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.57G/13.8G [01:49<03:04, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.58G/13.8G [01:50<03:52, 39.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.59G/13.8G [01:50<02:51, 53.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.60G/13.8G [01:50<03:40, 41.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  33% 4.61G/13.8G [01:50<03:46, 40.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.62G/13.8G [01:50<02:48, 54.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.63G/13.8G [01:51<03:23, 44.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.64G/13.8G [01:51<03:34, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.65G/13.8G [01:51<02:40, 56.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.66G/13.8G [01:51<02:54, 52.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.67G/13.8G [01:52<03:43, 40.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.69G/13.8G [01:52<02:46, 54.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.69G/13.8G [01:52<03:04, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.70G/13.8G [01:52<03:23, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.72G/13.8G [01:52<02:33, 59.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.73G/13.8G [01:53<03:28, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.74G/13.8G [01:53<04:24, 34.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  34% 4.75G/13.8G [01:53<03:12, 46.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.76G/13.8G [01:54<03:28, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.77G/13.8G [01:54<03:50, 39.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.78G/13.8G [01:54<02:50, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.79G/13.8G [01:54<03:25, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.80G/13.8G [01:54<03:32, 42.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.81G/13.8G [01:55<02:39, 56.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.82G/13.8G [01:55<02:51, 52.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.83G/13.8G [01:55<03:01, 49.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.85G/13.8G [01:55<02:19, 64.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.85G/13.8G [01:55<02:56, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.86G/13.8G [01:56<03:09, 47.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.88G/13.8G [01:56<02:24, 61.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  35% 4.89G/13.8G [01:56<02:53, 51.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.90G/13.8G [01:56<03:06, 47.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.91G/13.8G [01:56<02:22, 62.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.92G/13.8G [01:57<02:45, 53.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.93G/13.8G [01:57<03:02, 48.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.94G/13.8G [01:57<02:19, 63.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.95G/13.8G [01:57<02:46, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.96G/13.8G [01:57<02:58, 49.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.97G/13.8G [01:58<02:17, 64.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.98G/13.8G [01:58<02:42, 54.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 4.99G/13.8G [01:58<03:23, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 5.01G/13.8G [01:58<02:32, 57.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 5.01G/13.8G [01:58<02:54, 50.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  36% 5.02G/13.8G [01:59<03:24, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.04G/13.8G [01:59<02:33, 56.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.05G/13.8G [01:59<03:07, 46.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.06G/13.8G [01:59<03:13, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.07G/13.8G [01:59<02:25, 59.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.08G/13.8G [02:00<02:50, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.09G/13.8G [02:00<03:03, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.10G/13.8G [02:00<02:19, 62.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.11G/13.8G [02:00<02:42, 53.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.12G/13.8G [02:01<02:55, 49.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.13G/13.8G [02:01<02:14, 64.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.14G/13.8G [02:01<03:05, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  37% 5.15G/13.8G [02:01<03:24, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.17G/13.8G [02:01<02:32, 56.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.17G/13.8G [02:02<03:04, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.18G/13.8G [02:02<03:13, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.20G/13.8G [02:02<02:26, 58.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.21G/13.8G [02:02<03:02, 47.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.22G/13.8G [02:03<03:27, 41.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.23G/13.8G [02:03<02:34, 55.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.24G/13.8G [02:03<03:03, 46.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.25G/13.8G [02:03<03:21, 42.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.26G/13.8G [02:03<02:30, 56.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.27G/13.8G [02:04<02:55, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.28G/13.8G [02:04<03:00, 47.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.29G/13.8G [02:04<02:16, 62.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  38% 5.30G/13.8G [02:05<08:09, 17.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.31G/13.8G [02:06<07:01, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.33G/13.8G [02:06<04:48, 29.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.33G/13.8G [02:06<04:34, 30.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.34G/13.8G [02:06<04:24, 31.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.36G/13.8G [02:06<03:10, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.37G/13.8G [02:07<03:44, 37.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.38G/13.8G [02:07<03:46, 37.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.39G/13.8G [02:07<02:45, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.40G/13.8G [02:08<03:54, 35.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.41G/13.8G [02:08<03:47, 36.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.42G/13.8G [02:08<02:46, 50.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.43G/13.8G [02:08<03:09, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  39% 5.44G/13.8G [02:09<03:29, 39.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.45G/13.8G [02:09<02:35, 53.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.46G/13.8G [02:09<03:01, 45.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.47G/13.8G [02:09<03:06, 44.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.49G/13.8G [02:09<02:19, 59.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.49G/13.8G [02:09<02:28, 55.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.50G/13.8G [02:10<02:59, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.52G/13.8G [02:10<02:16, 60.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.53G/13.8G [02:10<02:36, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.54G/13.8G [02:10<03:01, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.55G/13.8G [02:10<02:17, 59.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.56G/13.8G [02:11<02:54, 47.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  40% 5.57G/13.8G [02:11<03:07, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.58G/13.8G [02:11<02:21, 58.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.59G/13.8G [02:11<02:38, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.60G/13.8G [02:12<02:44, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.61G/13.8G [02:12<02:06, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.62G/13.8G [02:12<02:29, 54.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.63G/13.8G [02:12<02:58, 45.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.65G/13.8G [02:12<02:15, 60.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.65G/13.8G [02:13<02:41, 50.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.66G/13.8G [02:13<02:53, 46.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.68G/13.8G [02:13<02:11, 61.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.69G/13.8G [02:13<02:34, 52.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.70G/13.8G [02:13<02:55, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  41% 5.71G/13.8G [02:14<02:13, 60.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.72G/13.8G [02:14<02:38, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.73G/13.8G [02:14<02:52, 46.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.74G/13.8G [02:14<02:11, 61.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.75G/13.8G [02:14<02:29, 53.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.76G/13.8G [02:15<02:55, 45.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.77G/13.8G [02:15<02:13, 60.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.78G/13.8G [02:15<02:38, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.79G/13.8G [02:15<03:25, 38.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.81G/13.8G [02:15<02:32, 52.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.81G/13.8G [02:16<02:55, 45.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.82G/13.8G [02:16<03:11, 41.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.84G/13.8G [02:16<02:23, 55.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  42% 5.85G/13.8G [02:16<02:44, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.86G/13.8G [02:17<03:07, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.87G/13.8G [02:17<02:20, 56.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.88G/13.8G [02:17<02:45, 47.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.89G/13.8G [02:17<03:05, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.90G/13.8G [02:17<02:18, 56.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.91G/13.8G [02:18<02:35, 50.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.92G/13.8G [02:18<02:47, 46.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.93G/13.8G [02:18<02:07, 61.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.94G/13.8G [02:18<02:37, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.95G/13.8G [02:19<02:45, 47.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.97G/13.8G [02:19<02:06, 61.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.97G/13.8G [02:19<02:17, 56.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  43% 5.98G/13.8G [02:19<02:46, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.00G/13.8G [02:19<02:06, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.01G/13.8G [02:20<02:51, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.02G/13.8G [02:20<03:03, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.03G/13.8G [02:20<02:17, 56.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.04G/13.8G [02:20<02:41, 47.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.05G/13.8G [02:20<02:51, 44.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.06G/13.8G [02:21<02:09, 59.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.07G/13.8G [02:21<02:41, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.08G/13.8G [02:21<03:10, 40.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.09G/13.8G [02:21<02:21, 54.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.10G/13.8G [02:22<02:59, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.11G/13.8G [02:22<03:03, 41.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  44% 6.13G/13.8G [02:22<02:17, 55.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.13G/13.8G [02:22<02:50, 44.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.14G/13.8G [02:23<02:58, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.16G/13.8G [02:23<02:13, 56.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.17G/13.8G [02:23<02:36, 48.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.18G/13.8G [02:23<02:48, 45.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.19G/13.8G [02:23<02:07, 59.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.20G/13.8G [02:24<02:50, 44.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.21G/13.8G [02:24<03:05, 40.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.22G/13.8G [02:24<02:18, 54.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.23G/13.8G [02:24<02:51, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.24G/13.8G [02:25<02:53, 43.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.25G/13.8G [02:25<02:10, 57.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  45% 6.26G/13.8G [02:25<02:38, 47.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.27G/13.8G [02:25<03:05, 40.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.29G/13.8G [02:25<02:17, 54.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.29G/13.8G [02:26<02:34, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.30G/13.8G [02:26<02:47, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.32G/13.8G [02:26<02:06, 58.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.33G/13.8G [02:26<02:32, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.34G/13.8G [02:26<02:42, 45.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.35G/13.8G [02:27<02:03, 60.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.36G/13.8G [02:27<02:25, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.37G/13.8G [02:27<02:29, 49.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.38G/13.8G [02:27<01:54, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.39G/13.8G [02:27<02:35, 47.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  46% 6.40G/13.8G [02:28<02:53, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.41G/13.8G [02:28<02:09, 56.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.42G/13.8G [02:28<02:37, 46.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.43G/13.8G [02:28<02:48, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.45G/13.8G [02:28<02:05, 58.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.45G/13.8G [02:29<02:16, 53.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.46G/13.8G [02:29<02:34, 47.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.48G/13.8G [02:29<01:57, 62.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.49G/13.8G [02:29<02:15, 53.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.50G/13.8G [02:30<02:39, 45.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.51G/13.8G [02:30<02:01, 59.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.52G/13.8G [02:30<02:20, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.53G/13.8G [02:30<02:34, 47.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  47% 6.54G/13.8G [02:30<01:57, 61.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.55G/13.8G [02:30<02:24, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.56G/13.8G [02:31<02:41, 44.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.57G/13.8G [02:31<02:01, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.58G/13.8G [02:31<02:09, 55.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.59G/13.8G [02:31<02:28, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.61G/13.8G [02:31<01:53, 62.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.61G/13.8G [02:32<02:12, 54.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.62G/13.8G [02:32<03:05, 38.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.64G/13.8G [02:32<02:16, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.65G/13.8G [02:32<02:26, 48.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.66G/13.8G [02:33<02:35, 45.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.67G/13.8G [02:33<02:35, 45.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.67G/13.8G [02:33<03:24, 34.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  48% 6.68G/13.8G [02:34<04:21, 27.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.69G/13.8G [02:34<03:00, 39.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.69G/13.8G [02:34<03:37, 32.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.70G/13.8G [02:34<03:20, 35.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.72G/13.8G [02:34<02:17, 51.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.73G/13.8G [02:34<02:37, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.74G/13.8G [02:35<02:56, 39.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.75G/13.8G [02:35<02:06, 55.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.76G/13.8G [02:35<02:25, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.77G/13.8G [02:35<02:37, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.78G/13.8G [02:36<01:55, 60.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.79G/13.8G [02:36<02:21, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.80G/13.8G [02:36<02:47, 41.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  49% 6.81G/13.8G [02:36<02:02, 56.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.82G/13.8G [02:36<02:20, 49.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.83G/13.8G [02:37<02:32, 45.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.85G/13.8G [02:37<01:53, 60.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.85G/13.8G [02:37<02:30, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.86G/13.8G [02:37<02:48, 40.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.88G/13.8G [02:38<02:04, 55.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.89G/13.8G [02:38<02:16, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.90G/13.8G [02:38<02:45, 41.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.91G/13.8G [02:38<02:02, 55.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.92G/13.8G [02:38<02:13, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.93G/13.8G [02:39<02:34, 44.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.94G/13.8G [02:39<01:55, 59.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  50% 6.95G/13.8G [02:39<02:29, 45.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 6.96G/13.8G [02:40<03:47, 30.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 6.97G/13.8G [02:40<02:42, 41.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 6.98G/13.8G [02:40<03:03, 37.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 6.99G/13.8G [02:40<02:54, 38.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 7.01G/13.8G [02:40<02:08, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 7.01G/13.8G [02:41<02:29, 45.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 7.02G/13.8G [02:41<02:57, 38.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 7.04G/13.8G [02:41<02:10, 51.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 7.05G/13.8G [02:41<02:19, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 7.06G/13.8G [02:42<02:38, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 7.07G/13.8G [02:42<01:58, 56.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 7.08G/13.8G [02:42<02:15, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  51% 7.09G/13.8G [02:42<02:21, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.10G/13.8G [02:42<01:47, 62.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.11G/13.8G [02:43<02:00, 55.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.12G/13.8G [02:43<02:17, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.13G/13.8G [02:43<01:44, 63.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.14G/13.8G [02:43<02:04, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.15G/13.8G [02:43<02:34, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.17G/13.8G [02:44<01:55, 57.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.17G/13.8G [02:44<02:11, 50.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.18G/13.8G [02:44<02:28, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.20G/13.8G [02:44<01:51, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.21G/13.8G [02:44<02:26, 44.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.22G/13.8G [02:45<02:41, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  52% 7.23G/13.8G [02:45<01:59, 54.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.24G/13.8G [02:45<02:25, 45.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.25G/13.8G [02:45<02:40, 40.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.26G/13.8G [02:46<02:35, 41.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.26G/13.8G [02:46<03:20, 32.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.27G/13.8G [02:46<04:03, 26.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.28G/13.8G [02:46<02:48, 38.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.28G/13.8G [02:47<03:12, 33.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.30G/13.8G [02:47<02:23, 45.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.30G/13.8G [02:47<02:45, 39.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.31G/13.8G [02:47<03:05, 34.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.33G/13.8G [02:47<02:09, 49.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.33G/13.8G [02:48<02:28, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.34G/13.8G [02:48<03:00, 35.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.36G/13.8G [02:48<02:09, 49.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  53% 7.37G/13.8G [02:48<02:21, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.38G/13.8G [02:49<02:28, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.39G/13.8G [02:49<01:51, 57.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.40G/13.8G [02:49<02:26, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.41G/13.8G [02:50<02:56, 36.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.42G/13.8G [02:50<02:09, 49.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.43G/13.8G [02:50<02:35, 40.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.44G/13.8G [02:50<02:48, 37.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.45G/13.8G [02:50<02:04, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.46G/13.8G [02:51<02:17, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.47G/13.8G [02:51<02:44, 38.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.49G/13.8G [02:51<02:01, 51.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.49G/13.8G [02:51<02:13, 47.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  54% 7.50G/13.8G [02:52<02:19, 44.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.52G/13.8G [02:52<01:45, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.53G/13.8G [02:52<02:07, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.54G/13.8G [02:52<02:13, 46.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.55G/13.8G [02:52<01:41, 61.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.56G/13.8G [02:53<02:06, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.57G/13.8G [02:53<02:19, 44.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.58G/13.8G [02:53<01:45, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.59G/13.8G [02:53<02:04, 49.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.60G/13.8G [02:53<02:19, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.61G/13.8G [02:54<01:45, 58.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.62G/13.8G [02:54<02:19, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  55% 7.63G/13.8G [02:55<03:46, 27.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.65G/13.8G [02:55<02:40, 38.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.65G/13.8G [02:55<03:12, 31.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.66G/13.8G [02:55<03:10, 32.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.68G/13.8G [02:56<02:17, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.69G/13.8G [02:56<02:19, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.70G/13.8G [02:56<02:29, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.71G/13.8G [02:56<01:50, 54.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.72G/13.8G [02:56<02:09, 46.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.73G/13.8G [02:57<02:26, 41.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.74G/13.8G [02:57<01:48, 55.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.75G/13.8G [02:57<02:00, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.76G/13.8G [02:57<02:15, 44.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.77G/13.8G [02:58<02:21, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  56% 7.78G/13.8G [02:58<03:10, 31.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.79G/13.8G [02:58<02:11, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.80G/13.8G [02:58<02:30, 39.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.81G/13.8G [02:59<02:38, 37.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.82G/13.8G [02:59<01:55, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.83G/13.8G [02:59<02:17, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.84G/13.8G [02:59<02:34, 38.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.85G/13.8G [02:59<01:53, 52.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.86G/13.8G [03:00<02:16, 43.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.87G/13.8G [03:00<02:41, 36.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.89G/13.8G [03:00<01:58, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.89G/13.8G [03:00<02:14, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.90G/13.8G [03:01<02:22, 41.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  57% 7.92G/13.8G [03:01<01:46, 54.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 7.93G/13.8G [03:01<02:12, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 7.94G/13.8G [03:01<02:19, 41.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 7.95G/13.8G [03:02<01:44, 55.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 7.96G/13.8G [03:02<02:13, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 7.97G/13.8G [03:02<02:34, 37.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 7.98G/13.8G [03:02<01:53, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 7.99G/13.8G [03:03<02:10, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 8.00G/13.8G [03:03<02:16, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 8.01G/13.8G [03:03<01:42, 56.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 8.02G/13.8G [03:03<02:06, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 8.03G/13.8G [03:03<02:16, 42.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 8.05G/13.8G [03:04<01:42, 56.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  58% 8.05G/13.8G [03:04<01:56, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.06G/13.8G [03:04<02:11, 43.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.08G/13.8G [03:04<01:38, 57.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.09G/13.8G [03:05<02:06, 44.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.10G/13.8G [03:05<02:12, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.11G/13.8G [03:05<01:39, 57.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.12G/13.8G [03:05<01:55, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.13G/13.8G [03:05<02:04, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.14G/13.8G [03:06<01:34, 59.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.15G/13.8G [03:06<01:52, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.16G/13.8G [03:06<02:51, 32.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.16G/13.8G [03:07<03:48, 24.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.17G/13.8G [03:07<02:31, 36.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.18G/13.8G [03:07<03:02, 30.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  59% 8.19G/13.8G [03:07<02:55, 31.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.21G/13.8G [03:07<02:03, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.21G/13.8G [03:08<02:12, 41.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.22G/13.8G [03:08<02:16, 40.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.24G/13.8G [03:08<01:40, 54.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.25G/13.8G [03:08<01:57, 46.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.26G/13.8G [03:09<02:34, 35.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.27G/13.8G [03:09<01:52, 48.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.28G/13.8G [03:09<02:20, 39.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.29G/13.8G [03:10<02:25, 37.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.30G/13.8G [03:10<01:47, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.31G/13.8G [03:10<02:05, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.32G/13.8G [03:10<03:01, 30.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  60% 8.32G/13.8G [03:11<03:45, 24.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.33G/13.8G [03:11<02:27, 36.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.34G/13.8G [03:11<02:35, 34.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.35G/13.8G [03:11<02:36, 34.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.37G/13.8G [03:11<01:51, 48.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.37G/13.8G [03:12<02:05, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.38G/13.8G [03:12<02:28, 36.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.40G/13.8G [03:12<01:48, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.41G/13.8G [03:12<01:56, 45.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.42G/13.8G [03:13<02:09, 41.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.43G/13.8G [03:13<01:36, 55.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.44G/13.8G [03:13<01:54, 46.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.45G/13.8G [03:13<02:08, 41.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.46G/13.8G [03:14<01:35, 55.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  61% 8.47G/13.8G [03:14<01:48, 48.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.48G/13.8G [03:14<01:56, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.49G/13.8G [03:14<01:28, 59.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.50G/13.8G [03:14<01:43, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.51G/13.8G [03:15<01:58, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.53G/13.8G [03:15<01:29, 58.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.53G/13.8G [03:15<01:49, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.54G/13.8G [03:15<02:07, 41.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.56G/13.8G [03:15<01:35, 54.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.57G/13.8G [03:16<02:02, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.58G/13.8G [03:16<02:07, 40.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.59G/13.8G [03:16<01:35, 54.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  62% 8.60G/13.8G [03:16<01:48, 47.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.61G/13.8G [03:17<01:56, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.62G/13.8G [03:17<01:27, 58.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.63G/13.8G [03:17<01:58, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.64G/13.8G [03:17<02:00, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.65G/13.8G [03:17<01:30, 56.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.66G/13.8G [03:18<01:44, 48.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.67G/13.8G [03:18<01:55, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.69G/13.8G [03:18<01:26, 58.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.69G/13.8G [03:18<01:40, 50.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.70G/13.8G [03:19<01:50, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.72G/13.8G [03:19<01:23, 60.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.73G/13.8G [03:19<02:43, 30.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  63% 8.74G/13.8G [03:20<02:30, 33.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.75G/13.8G [03:20<01:49, 45.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.76G/13.8G [03:20<02:01, 41.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.77G/13.8G [03:20<02:05, 40.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.78G/13.8G [03:20<01:32, 53.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.79G/13.8G [03:21<01:47, 46.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.80G/13.8G [03:21<01:57, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.81G/13.8G [03:21<01:27, 56.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.82G/13.8G [03:21<01:35, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.83G/13.8G [03:22<01:53, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.85G/13.8G [03:22<01:25, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.85G/13.8G [03:22<01:34, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.86G/13.8G [03:22<01:38, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  64% 8.88G/13.8G [03:22<01:15, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.89G/13.8G [03:22<01:30, 53.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.90G/13.8G [03:23<01:49, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.91G/13.8G [03:23<01:22, 58.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.92G/13.8G [03:23<01:40, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.93G/13.8G [03:23<01:47, 45.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.94G/13.8G [03:23<01:21, 59.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.95G/13.8G [03:24<01:32, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.96G/13.8G [03:24<01:43, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.97G/13.8G [03:24<01:18, 61.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.98G/13.8G [03:24<01:26, 55.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 8.99G/13.8G [03:25<01:38, 48.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 9.01G/13.8G [03:25<01:15, 63.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  65% 9.01G/13.8G [03:25<01:34, 50.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.02G/13.8G [03:25<01:47, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.04G/13.8G [03:25<01:20, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.05G/13.8G [03:26<01:35, 49.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.06G/13.8G [03:26<01:46, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.07G/13.8G [03:26<01:20, 58.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.08G/13.8G [03:26<01:39, 47.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.09G/13.8G [03:27<02:39, 29.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.10G/13.8G [03:27<01:53, 41.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.11G/13.8G [03:27<02:00, 38.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.12G/13.8G [03:27<02:01, 38.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.13G/13.8G [03:28<01:29, 52.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.14G/13.8G [03:28<01:37, 47.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  66% 9.15G/13.8G [03:28<01:44, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.17G/13.8G [03:28<01:18, 58.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.17G/13.8G [03:28<01:27, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.18G/13.8G [03:29<01:40, 45.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.20G/13.8G [03:29<01:16, 60.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.21G/13.8G [03:29<01:33, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.22G/13.8G [03:29<01:43, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.23G/13.8G [03:29<01:18, 58.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.24G/13.8G [03:30<01:28, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.25G/13.8G [03:30<01:34, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.26G/13.8G [03:30<01:11, 62.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.27G/13.8G [03:30<01:39, 45.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.28G/13.8G [03:31<01:45, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  67% 9.29G/13.8G [03:31<01:19, 56.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.30G/13.8G [03:31<01:36, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.31G/13.8G [03:31<01:51, 40.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.33G/13.8G [03:31<01:22, 53.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.33G/13.8G [03:32<01:37, 45.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.34G/13.8G [03:32<01:40, 43.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.36G/13.8G [03:32<01:15, 58.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.37G/13.8G [03:32<01:23, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.38G/13.8G [03:33<01:40, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.39G/13.8G [03:33<01:15, 58.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.40G/13.8G [03:33<01:31, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.41G/13.8G [03:33<01:36, 45.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.42G/13.8G [03:33<01:12, 59.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  68% 9.43G/13.8G [03:33<01:21, 53.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.44G/13.8G [03:34<01:33, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.45G/13.8G [03:34<01:10, 61.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.46G/13.8G [03:34<01:22, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.47G/13.8G [03:35<01:52, 38.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.49G/13.8G [03:35<01:22, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.49G/13.8G [03:35<01:45, 40.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.50G/13.8G [03:35<01:40, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.52G/13.8G [03:35<01:14, 56.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.53G/13.8G [03:36<01:26, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.54G/13.8G [03:36<01:28, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.55G/13.8G [03:36<01:07, 62.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.56G/13.8G [03:36<01:17, 54.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  69% 9.57G/13.8G [03:36<01:36, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.58G/13.8G [03:37<01:12, 58.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.59G/13.8G [03:37<01:25, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.60G/13.8G [03:37<01:30, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.61G/13.8G [03:37<01:08, 60.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.62G/13.8G [03:37<01:30, 45.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.63G/13.8G [03:38<01:38, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.65G/13.8G [03:38<01:13, 56.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.65G/13.8G [03:38<01:22, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.66G/13.8G [03:38<01:34, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.68G/13.8G [03:38<01:11, 57.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.69G/13.8G [03:39<01:26, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.70G/13.8G [03:39<01:31, 44.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  70% 9.71G/13.8G [03:39<01:08, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.72G/13.8G [03:39<01:20, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.73G/13.8G [03:40<01:24, 47.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.74G/13.8G [03:40<01:04, 62.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.75G/13.8G [03:40<01:24, 47.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.76G/13.8G [03:40<01:31, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.77G/13.8G [03:40<01:08, 58.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.78G/13.8G [03:41<01:17, 51.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.79G/13.8G [03:41<01:30, 43.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.81G/13.8G [03:41<01:08, 58.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.81G/13.8G [03:41<01:20, 49.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.82G/13.8G [03:42<01:35, 41.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.84G/13.8G [03:42<01:11, 55.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  71% 9.85G/13.8G [03:42<01:25, 45.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.86G/13.8G [03:42<01:31, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.87G/13.8G [03:42<01:08, 56.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.88G/13.8G [03:43<01:20, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.89G/13.8G [03:43<01:41, 38.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.90G/13.8G [03:43<01:14, 51.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.91G/13.8G [03:43<01:27, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.92G/13.8G [03:44<01:29, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.93G/13.8G [03:44<01:06, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.94G/13.8G [03:44<01:20, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.95G/13.8G [03:44<01:33, 40.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.97G/13.8G [03:44<01:09, 54.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.97G/13.8G [03:45<01:25, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  72% 9.98G/13.8G [03:45<01:27, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.0G/13.8G [03:45<01:05, 57.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.0G/13.8G [03:45<01:15, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.0G/13.8G [03:46<02:33, 24.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.0G/13.8G [03:46<01:46, 35.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.0G/13.8G [03:47<01:55, 32.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.0G/13.8G [03:47<01:56, 32.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.1G/13.8G [03:47<01:23, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.1G/13.8G [03:47<01:32, 40.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.1G/13.8G [03:48<01:34, 39.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.1G/13.8G [03:48<01:09, 53.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.1G/13.8G [03:48<01:16, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  73% 10.1G/13.8G [03:48<01:34, 38.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.1G/13.8G [03:48<01:09, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.1G/13.8G [03:49<01:20, 45.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.1G/13.8G [03:49<01:27, 41.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.2G/13.8G [03:49<01:05, 55.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.2G/13.8G [03:49<01:14, 48.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.2G/13.8G [03:50<01:20, 44.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.2G/13.8G [03:50<01:00, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.2G/13.8G [03:50<01:13, 48.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.2G/13.8G [03:50<01:16, 46.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.2G/13.8G [03:50<00:58, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.2G/13.8G [03:50<01:09, 50.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.2G/13.8G [03:51<01:16, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  74% 10.3G/13.8G [03:51<00:57, 61.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.3G/13.8G [03:51<01:05, 53.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.3G/13.8G [03:51<01:20, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.3G/13.8G [03:51<01:00, 57.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.3G/13.8G [03:52<01:09, 50.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.3G/13.8G [03:52<01:21, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.3G/13.8G [03:52<01:00, 56.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.3G/13.8G [03:52<01:17, 44.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.3G/13.8G [03:53<01:23, 41.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.3G/13.8G [03:53<01:01, 55.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.4G/13.8G [03:53<01:13, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.4G/13.8G [03:53<01:21, 42.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.4G/13.8G [03:54<01:00, 56.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  75% 10.4G/13.8G [03:54<01:08, 49.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.4G/13.8G [03:54<01:18, 43.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.4G/13.8G [03:54<00:58, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.4G/13.8G [03:55<01:29, 37.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.4G/13.8G [03:55<01:25, 38.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.4G/13.8G [03:55<01:03, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.5G/13.8G [03:55<01:07, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.5G/13.8G [03:55<01:11, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.5G/13.8G [03:55<00:54, 60.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.5G/13.8G [03:56<01:07, 48.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.5G/13.8G [03:56<01:16, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.5G/13.8G [03:56<00:57, 57.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.5G/13.8G [03:56<01:06, 48.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  76% 10.5G/13.8G [03:57<01:17, 41.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.5G/13.8G [03:57<00:58, 55.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.5G/13.8G [03:57<01:06, 48.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.6G/13.8G [03:57<01:12, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.6G/13.8G [03:57<00:54, 58.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.6G/13.8G [03:58<01:02, 50.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.6G/13.8G [03:58<01:13, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.6G/13.8G [03:58<00:55, 57.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.6G/13.8G [03:58<01:16, 41.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.6G/13.8G [03:59<01:19, 39.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.6G/13.8G [03:59<00:58, 53.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.6G/13.8G [03:59<01:14, 42.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.7G/13.8G [03:59<01:19, 39.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  77% 10.7G/13.8G [04:00<00:58, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.7G/13.8G [04:00<01:05, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.7G/13.8G [04:00<01:16, 40.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.7G/13.8G [04:00<00:56, 54.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.7G/13.8G [04:01<01:06, 46.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.7G/13.8G [04:01<01:05, 46.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.7G/13.8G [04:01<00:49, 61.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.7G/13.8G [04:01<01:08, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.8G/13.8G [04:02<01:34, 32.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.8G/13.8G [04:02<01:07, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.8G/13.8G [04:02<01:14, 40.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.8G/13.8G [04:02<01:13, 40.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.8G/13.8G [04:02<00:54, 54.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  78% 10.8G/13.8G [04:03<00:59, 49.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.8G/13.8G [04:03<01:11, 41.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.8G/13.8G [04:03<00:53, 55.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.8G/13.8G [04:03<00:55, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.8G/13.8G [04:04<01:04, 45.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.9G/13.8G [04:04<00:48, 59.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.9G/13.8G [04:04<00:57, 50.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.9G/13.8G [04:04<01:09, 41.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.9G/13.8G [04:04<00:52, 55.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.9G/13.8G [04:05<00:59, 47.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.9G/13.8G [04:05<01:22, 34.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.9G/13.8G [04:05<01:00, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.9G/13.8G [04:05<01:09, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  79% 10.9G/13.8G [04:06<01:16, 36.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.0G/13.8G [04:06<00:56, 50.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.0G/13.8G [04:06<01:02, 44.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.0G/13.8G [04:06<01:10, 39.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.0G/13.8G [04:07<00:52, 53.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.0G/13.8G [04:07<00:55, 49.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.0G/13.8G [04:07<01:02, 44.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.0G/13.8G [04:07<00:46, 58.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.0G/13.8G [04:07<00:58, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.0G/13.8G [04:08<01:03, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.1G/13.8G [04:08<00:47, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.1G/13.8G [04:08<00:53, 50.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.1G/13.8G [04:08<01:02, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  80% 11.1G/13.8G [04:08<00:46, 57.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.1G/13.8G [04:09<00:54, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.1G/13.8G [04:09<00:58, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.1G/13.8G [04:09<00:44, 60.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.1G/13.8G [04:09<00:52, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.1G/13.8G [04:10<01:04, 41.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.1G/13.8G [04:10<00:47, 55.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.2G/13.8G [04:10<00:56, 46.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.2G/13.8G [04:10<00:59, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.2G/13.8G [04:10<00:44, 58.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.2G/13.8G [04:11<00:54, 47.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.2G/13.8G [04:11<01:01, 41.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.2G/13.8G [04:11<00:45, 56.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  81% 11.2G/13.8G [04:11<00:54, 46.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.2G/13.8G [04:12<00:55, 46.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.2G/13.8G [04:12<00:41, 60.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:12<00:59, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:12<00:59, 42.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:12<00:44, 56.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:13<00:55, 45.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:13<00:59, 41.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:13<00:44, 55.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:13<00:49, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:14<00:56, 43.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:14<00:42, 57.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.3G/13.8G [04:14<00:46, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  82% 11.4G/13.8G [04:14<00:49, 48.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.4G/13.8G [04:14<00:37, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.4G/13.8G [04:15<00:47, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.4G/13.8G [04:15<00:52, 45.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.4G/13.8G [04:15<00:39, 60.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.4G/13.8G [04:15<00:44, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.4G/13.8G [04:15<00:45, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.4G/13.8G [04:15<00:34, 66.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.4G/13.8G [04:16<00:40, 57.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.5G/13.8G [04:16<00:51, 44.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.5G/13.8G [04:16<00:38, 59.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.5G/13.8G [04:16<00:45, 50.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  83% 11.5G/13.8G [04:17<00:51, 44.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.5G/13.8G [04:17<00:38, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.5G/13.8G [04:17<00:43, 51.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.5G/13.8G [04:17<01:03, 35.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.5G/13.8G [04:18<00:46, 48.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.5G/13.8G [04:18<00:49, 45.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.6G/13.8G [04:18<00:51, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.6G/13.8G [04:18<00:38, 57.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.6G/13.8G [04:18<00:47, 46.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.6G/13.8G [04:19<01:00, 36.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.6G/13.8G [04:19<00:43, 49.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.6G/13.8G [04:19<00:47, 45.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.6G/13.8G [04:19<00:49, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  84% 11.6G/13.8G [04:20<00:37, 57.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.6G/13.8G [04:20<00:42, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.6G/13.8G [04:20<00:50, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.7G/13.8G [04:20<00:37, 56.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.7G/13.8G [04:21<00:49, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.7G/13.8G [04:21<00:50, 41.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.7G/13.8G [04:21<00:37, 55.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.7G/13.8G [04:21<00:41, 50.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.7G/13.8G [04:21<00:46, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.7G/13.8G [04:22<00:35, 58.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.7G/13.8G [04:22<00:42, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.7G/13.8G [04:22<00:45, 44.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.8G/13.8G [04:22<00:34, 59.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  85% 11.8G/13.8G [04:22<00:39, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.8G/13.8G [04:23<00:45, 43.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.8G/13.8G [04:23<00:33, 58.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.8G/13.8G [04:23<00:37, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.8G/13.8G [04:23<00:43, 45.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.8G/13.8G [04:23<00:32, 59.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.8G/13.8G [04:24<00:37, 52.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.8G/13.8G [04:24<00:41, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.9G/13.8G [04:24<00:31, 61.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.9G/13.8G [04:24<00:38, 49.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.9G/13.8G [04:25<00:43, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.9G/13.8G [04:25<00:32, 58.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.9G/13.8G [04:25<00:37, 50.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  86% 11.9G/13.8G [04:25<00:41, 44.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 11.9G/13.8G [04:25<00:31, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 11.9G/13.8G [04:25<00:34, 53.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 11.9G/13.8G [04:26<00:52, 35.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:26<00:37, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:26<00:43, 41.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:27<00:45, 39.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:27<00:33, 53.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:27<00:36, 48.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:27<00:40, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:27<00:29, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:28<00:36, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:28<00:40, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  87% 12.0G/13.8G [04:28<00:30, 57.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.1G/13.8G [04:28<00:35, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.1G/13.8G [04:28<00:36, 47.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.1G/13.8G [04:29<00:27, 61.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.1G/13.8G [04:29<00:33, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.1G/13.8G [04:29<00:42, 39.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.1G/13.8G [04:29<00:31, 53.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.1G/13.8G [04:30<00:34, 47.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.1G/13.8G [04:30<00:37, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.1G/13.8G [04:30<00:27, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.2G/13.8G [04:30<00:30, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.2G/13.8G [04:30<00:33, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.2G/13.8G [04:31<00:25, 63.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  88% 12.2G/13.8G [04:31<00:33, 46.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.2G/13.8G [04:31<00:40, 39.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.2G/13.8G [04:31<00:29, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.2G/13.8G [04:32<00:33, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.2G/13.8G [04:32<00:36, 42.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.2G/13.8G [04:32<00:27, 56.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.2G/13.8G [04:32<00:31, 48.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.3G/13.8G [04:32<00:32, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.3G/13.8G [04:32<00:24, 61.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.3G/13.8G [04:33<00:30, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.3G/13.8G [04:33<00:34, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.3G/13.8G [04:33<00:25, 58.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.3G/13.8G [04:33<00:28, 50.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  89% 12.3G/13.8G [04:34<00:32, 44.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.3G/13.8G [04:34<00:24, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.3G/13.8G [04:34<00:31, 45.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:34<00:31, 44.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:34<00:23, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:35<00:30, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:35<00:35, 39.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:35<00:26, 52.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:35<00:28, 47.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:36<00:32, 41.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:36<00:23, 56.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:36<00:29, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.4G/13.8G [04:36<00:31, 41.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  90% 12.5G/13.8G [04:37<00:23, 55.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.5G/13.8G [04:37<00:27, 48.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.5G/13.8G [04:37<00:40, 32.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.5G/13.8G [04:38<00:52, 24.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.5G/13.8G [04:38<00:34, 36.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.5G/13.8G [04:38<00:39, 31.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.5G/13.8G [04:38<00:35, 35.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.5G/13.8G [04:38<00:25, 49.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.5G/13.8G [04:39<00:26, 47.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.5G/13.8G [04:39<00:28, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.6G/13.8G [04:39<00:21, 57.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.6G/13.8G [04:39<00:24, 48.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.6G/13.8G [04:40<00:29, 41.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.6G/13.8G [04:40<00:21, 55.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  91% 12.6G/13.8G [04:40<00:25, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.6G/13.8G [04:40<00:25, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.6G/13.8G [04:40<00:19, 59.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.6G/13.8G [04:41<00:25, 45.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.6G/13.8G [04:41<00:25, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.7G/13.8G [04:41<00:19, 58.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.7G/13.8G [04:41<00:21, 51.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.7G/13.8G [04:41<00:25, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.7G/13.8G [04:42<00:18, 58.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.7G/13.8G [04:42<00:24, 44.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.7G/13.8G [04:42<00:28, 37.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.7G/13.8G [04:42<00:20, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.7G/13.8G [04:43<00:22, 45.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  92% 12.7G/13.8G [04:43<00:23, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.7G/13.8G [04:43<00:17, 58.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.8G/13.8G [04:43<00:19, 51.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.8G/13.8G [04:43<00:20, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.8G/13.8G [04:43<00:15, 62.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.8G/13.8G [04:44<00:17, 56.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.8G/13.8G [04:44<00:22, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.8G/13.8G [04:44<00:16, 57.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.8G/13.8G [04:44<00:20, 46.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.8G/13.8G [04:45<00:21, 44.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.8G/13.8G [04:45<00:15, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.9G/13.8G [04:45<00:18, 50.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  93% 12.9G/13.8G [04:45<00:18, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 12.9G/13.8G [04:45<00:14, 62.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 12.9G/13.8G [04:46<00:17, 51.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 12.9G/13.8G [04:46<00:18, 47.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 12.9G/13.8G [04:46<00:13, 62.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 12.9G/13.8G [04:46<00:16, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 12.9G/13.8G [04:46<00:18, 44.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 12.9G/13.8G [04:47<00:14, 59.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 12.9G/13.8G [04:47<00:16, 48.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 13.0G/13.8G [04:47<00:19, 41.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 13.0G/13.8G [04:47<00:14, 55.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 13.0G/13.8G [04:48<00:17, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 13.0G/13.8G [04:48<00:18, 41.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 13.0G/13.8G [04:48<00:13, 55.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  94% 13.0G/13.8G [04:48<00:16, 47.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.0G/13.8G [04:49<00:17, 42.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.0G/13.8G [04:49<00:13, 56.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.0G/13.8G [04:49<00:14, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.1G/13.8G [04:49<00:17, 41.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.1G/13.8G [04:49<00:12, 55.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.1G/13.8G [04:50<00:14, 46.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.1G/13.8G [04:50<00:15, 45.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.1G/13.8G [04:50<00:11, 59.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.1G/13.8G [04:50<00:14, 44.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.1G/13.8G [04:50<00:15, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.1G/13.8G [04:51<00:11, 56.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.1G/13.8G [04:51<00:12, 49.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  95% 13.2G/13.8G [04:51<00:14, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.2G/13.8G [04:51<00:10, 57.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.2G/13.8G [04:52<00:13, 44.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.2G/13.8G [04:52<00:13, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.2G/13.8G [04:52<00:10, 56.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.2G/13.8G [04:52<00:13, 43.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.2G/13.8G [04:53<00:14, 38.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.2G/13.8G [04:53<00:10, 52.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.2G/13.8G [04:53<00:11, 47.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.2G/13.8G [04:53<00:11, 46.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.3G/13.8G [04:53<00:08, 61.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.3G/13.8G [04:53<00:09, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  96% 13.3G/13.8G [04:54<00:13, 37.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.3G/13.8G [04:54<00:09, 50.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.3G/13.8G [04:54<00:10, 45.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.3G/13.8G [04:55<00:10, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.3G/13.8G [04:55<00:07, 57.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.3G/13.8G [04:55<00:08, 49.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.3G/13.8G [04:55<00:09, 45.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.4G/13.8G [04:55<00:06, 60.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.4G/13.8G [04:56<00:09, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.4G/13.8G [04:56<00:10, 38.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.4G/13.8G [04:56<00:07, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.4G/13.8G [04:56<00:07, 47.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.4G/13.8G [04:56<00:08, 44.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  97% 13.4G/13.8G [04:57<00:05, 59.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.4G/13.8G [04:58<00:14, 23.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.4G/13.8G [04:58<00:13, 24.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [04:58<00:08, 35.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [04:58<00:08, 35.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [04:59<00:08, 35.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [04:59<00:05, 49.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [04:59<00:06, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [04:59<00:06, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [04:59<00:04, 56.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [05:00<00:05, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [05:00<00:07, 31.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [05:00<00:09, 25.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.5G/13.8G [05:00<00:06, 37.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  98% 13.6G/13.8G [05:01<00:06, 31.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.6G/13.8G [05:01<00:06, 32.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.6G/13.8G [05:01<00:04, 46.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.6G/13.8G [05:01<00:04, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.6G/13.8G [05:02<00:04, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.6G/13.8G [05:02<00:02, 54.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.6G/13.8G [05:02<00:03, 47.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.6G/13.8G [05:02<00:03, 45.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.6G/13.8G [05:02<00:02, 60.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.7G/13.8G [05:03<00:02, 46.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.7G/13.8G [05:03<00:02, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.7G/13.8G [05:03<00:01, 54.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.7G/13.8G [05:03<00:01, 50.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf:  99% 13.7G/13.8G [05:04<00:01, 45.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf: 100% 13.7G/13.8G [05:04<00:01, 60.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf: 100% 13.7G/13.8G [05:04<00:01, 53.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf: 100% 13.7G/13.8G [05:04<00:00, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf: 100% 13.7G/13.8G [05:04<00:00, 62.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf: 100% 13.7G/13.8G [05:05<00:00, 53.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "llama-7b-alpaca-light-f16.gguf: 100% 13.8G/13.8G [05:05<00:00, 45.0MB/s]\n",
            "\n",
            "Upload 2 LFS files: 100% 2/2 [05:06<00:00, 153.20s/it]\n",
            "https://huggingface.co/LightXXXXX/llama-7b-alpaca-light-gguf/tree/main/.\n"
          ]
        }
      ]
    }
  ]
}