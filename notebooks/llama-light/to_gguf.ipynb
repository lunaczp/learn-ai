{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunaczp/learn-ai/blob/main/notebooks/llama-light/to_gguf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtLstdXUR3kV"
      },
      "source": [
        "# Convert pytorch model to gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "sCMCeoMgR3kZ",
        "outputId": "d328408e-185e-4fb9-8cfa-87d52b1b8806",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.11.0)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2024.2.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "# installing the huggingface_hub\n",
        "! pip install -U \"huggingface_hub[cli]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7liC1lkR3ka"
      },
      "outputs": [],
      "source": [
        "# login\n",
        "! huggingface-cli login\n",
        "# download model\n",
        "! huggingface-cli download \"LightXXXXX/llama-2-7b-light\" --local-dir llama-2-7b-light/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "p65l9TD1R3kb",
        "outputId": "2b38d233-9edf-4b81-8cd8-cf6d7d7ac017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 24577, done.\u001b[K\n",
            "remote: Counting objects: 100% (5406/5406), done.\u001b[K\n",
            "remote: Compressing objects: 100% (227/227), done.\u001b[K\n",
            "remote: Total 24577 (delta 5277), reused 5198 (delta 5178), pack-reused 19171\u001b[K\n",
            "Receiving objects: 100% (24577/24577), 43.69 MiB | 19.90 MiB/s, done.\n",
            "Resolving deltas: 100% (17434/17434), done.\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'reqiurement.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# download llama.cpp\n",
        "! git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "! pip install -r requirements.txt\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ms5JZ86UR3kb",
        "outputId": "4ff42e9c-62f3-4b88-8acf-752addfbde86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘models’: File exists\n",
            "INFO:convert:Loading model file llama-2-7b-light/pytorch_model-00001-of-00002.bin\n",
            "INFO:convert:Loading model file llama-2-7b-light/pytorch_model-00001-of-00002.bin\n",
            "INFO:convert:Loading model file llama-2-7b-light/pytorch_model-00002-of-00002.bin\n",
            "INFO:convert:model parameters count : 6738417664 (7B)\n",
            "INFO:convert:params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('llama-2-7b-light'))\n",
            "INFO:convert:Loaded vocab file PosixPath('llama-2-7b-light/tokenizer.model'), type 'spm'\n",
            "INFO:convert:Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
            "INFO:convert:Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 0}, add special tokens unset>\n",
            "INFO:convert:Writing models/llama-2-7b-light-f16.gguf, format 1\n",
            "WARNING:convert:Ignoring added_tokens.json since model matches vocab size without it.\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:convert:[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   6\n",
            "INFO:convert:[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   7\n",
            "INFO:convert:[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   9\n",
            "INFO:convert:[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   9\n",
            "INFO:convert:[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   9\n",
            "INFO:convert:[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  10\n",
            "INFO:convert:[  7/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  10\n",
            "INFO:convert:[  8/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  10\n",
            "INFO:convert:[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  14\n",
            "INFO:convert:[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  14\n",
            "INFO:convert:[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  14\n",
            "INFO:convert:[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  14\n",
            "INFO:convert:[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  15\n",
            "INFO:convert:[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+  15\n",
            "INFO:convert:[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  15\n",
            "INFO:convert:[ 16/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  15\n",
            "INFO:convert:[ 17/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  20\n",
            "INFO:convert:[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  20\n",
            "INFO:convert:[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  20\n",
            "INFO:convert:[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  20\n",
            "INFO:convert:[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  20\n",
            "INFO:convert:[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  20\n",
            "INFO:convert:[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+  20\n",
            "INFO:convert:[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  20\n",
            "INFO:convert:[ 25/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  24\n",
            "INFO:convert:[ 26/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  24\n",
            "INFO:convert:[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  25\n",
            "INFO:convert:[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  25\n",
            "INFO:convert:[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  25\n",
            "INFO:convert:[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  25\n",
            "INFO:convert:[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  25\n",
            "INFO:convert:[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+  25\n",
            "INFO:convert:[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  25\n",
            "INFO:convert:[ 34/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  26\n",
            "INFO:convert:[ 35/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  27\n",
            "INFO:convert:[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  27\n",
            "INFO:convert:[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  27\n",
            "INFO:convert:[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  27\n",
            "INFO:convert:[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  28\n",
            "INFO:convert:[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  29\n",
            "INFO:convert:[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  29\n",
            "INFO:convert:[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  29\n",
            "INFO:convert:[ 43/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  30\n",
            "INFO:convert:[ 44/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  32\n",
            "INFO:convert:[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  32\n",
            "INFO:convert:[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  32\n",
            "INFO:convert:[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  32\n",
            "INFO:convert:[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  33\n",
            "INFO:convert:[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[ 52/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  34\n",
            "INFO:convert:[ 53/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  35\n",
            "INFO:convert:[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  39\n",
            "INFO:convert:[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  39\n",
            "INFO:convert:[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  39\n",
            "INFO:convert:[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  39\n",
            "INFO:convert:[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  39\n",
            "INFO:convert:[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  40\n",
            "INFO:convert:[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  40\n",
            "INFO:convert:[ 61/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  40\n",
            "INFO:convert:[ 62/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  44\n",
            "INFO:convert:[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  45\n",
            "INFO:convert:[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  45\n",
            "INFO:convert:[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  45\n",
            "INFO:convert:[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  45\n",
            "INFO:convert:[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  45\n",
            "INFO:convert:[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  46\n",
            "INFO:convert:[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  49\n",
            "INFO:convert:[ 70/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  50\n",
            "INFO:convert:[ 71/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  50\n",
            "INFO:convert:[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  50\n",
            "INFO:convert:[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  50\n",
            "INFO:convert:[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  50\n",
            "INFO:convert:[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  50\n",
            "INFO:convert:[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  51\n",
            "INFO:convert:[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  51\n",
            "INFO:convert:[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  51\n",
            "INFO:convert:[ 79/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  53\n",
            "INFO:convert:[ 80/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  53\n",
            "INFO:convert:[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  53\n",
            "INFO:convert:[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  53\n",
            "INFO:convert:[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  53\n",
            "INFO:convert:[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  53\n",
            "INFO:convert:[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  54\n",
            "INFO:convert:[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  54\n",
            "INFO:convert:[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  55\n",
            "INFO:convert:[ 88/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  56\n",
            "INFO:convert:[ 89/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  57\n",
            "INFO:convert:[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  57\n",
            "INFO:convert:[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  57\n",
            "INFO:convert:[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  57\n",
            "INFO:convert:[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  57\n",
            "INFO:convert:[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  57\n",
            "INFO:convert:[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  58\n",
            "INFO:convert:[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  58\n",
            "INFO:convert:[ 97/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  61\n",
            "INFO:convert:[ 98/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  61\n",
            "INFO:convert:[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  61\n",
            "INFO:convert:[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  61\n",
            "INFO:convert:[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  61\n",
            "INFO:convert:[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  61\n",
            "INFO:convert:[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  61\n",
            "INFO:convert:[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  61\n",
            "INFO:convert:[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  62\n",
            "INFO:convert:[106/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  64\n",
            "INFO:convert:[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  65\n",
            "INFO:convert:[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  65\n",
            "INFO:convert:[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  65\n",
            "INFO:convert:[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  65\n",
            "INFO:convert:[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  65\n",
            "INFO:convert:[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  65\n",
            "INFO:convert:[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  65\n",
            "INFO:convert:[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  66\n",
            "INFO:convert:[115/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  66\n",
            "INFO:convert:[116/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  67\n",
            "INFO:convert:[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  68\n",
            "INFO:convert:[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  68\n",
            "INFO:convert:[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  68\n",
            "INFO:convert:[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  69\n",
            "INFO:convert:[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  69\n",
            "INFO:convert:[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  69\n",
            "INFO:convert:[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  71\n",
            "INFO:convert:[124/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  71\n",
            "INFO:convert:[125/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  71\n",
            "INFO:convert:[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
            "INFO:convert:[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  72\n",
            "INFO:convert:[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  72\n",
            "INFO:convert:[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  72\n",
            "INFO:convert:[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  72\n",
            "INFO:convert:[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  72\n",
            "INFO:convert:[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  73\n",
            "INFO:convert:[133/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  74\n",
            "INFO:convert:[134/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  75\n",
            "INFO:convert:[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  75\n",
            "INFO:convert:[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  75\n",
            "INFO:convert:[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  75\n",
            "INFO:convert:[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  75\n",
            "INFO:convert:[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  76\n",
            "INFO:convert:[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  76\n",
            "INFO:convert:[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  77\n",
            "INFO:convert:[142/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  77\n",
            "INFO:convert:[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  79\n",
            "INFO:convert:[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  79\n",
            "INFO:convert:[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  79\n",
            "INFO:convert:[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  79\n",
            "INFO:convert:[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  80\n",
            "INFO:convert:[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  80\n",
            "INFO:convert:[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  80\n",
            "INFO:convert:[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  80\n",
            "INFO:convert:[151/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  84\n",
            "INFO:convert:[152/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  84\n",
            "INFO:convert:[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  85\n",
            "INFO:convert:[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  85\n",
            "INFO:convert:[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  85\n",
            "INFO:convert:[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  85\n",
            "INFO:convert:[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  85\n",
            "INFO:convert:[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  85\n",
            "INFO:convert:[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  85\n",
            "INFO:convert:[160/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  86\n",
            "INFO:convert:[161/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  87\n",
            "INFO:convert:[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  87\n",
            "INFO:convert:[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  90\n",
            "INFO:convert:[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  90\n",
            "INFO:convert:[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  90\n",
            "INFO:convert:[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  90\n",
            "INFO:convert:[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  90\n",
            "INFO:convert:[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  90\n",
            "INFO:convert:[169/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  90\n",
            "INFO:convert:[170/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  91\n",
            "INFO:convert:[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  95\n",
            "INFO:convert:[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  95\n",
            "INFO:convert:[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  95\n",
            "INFO:convert:[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  95\n",
            "INFO:convert:[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  95\n",
            "INFO:convert:[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  95\n",
            "INFO:convert:[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  96\n",
            "INFO:convert:[178/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 100\n",
            "INFO:convert:[179/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 100\n",
            "INFO:convert:[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 101\n",
            "INFO:convert:[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 101\n",
            "INFO:convert:[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 101\n",
            "INFO:convert:[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 101\n",
            "INFO:convert:[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 105\n",
            "INFO:convert:[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 105\n",
            "INFO:convert:[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 105\n",
            "INFO:convert:[187/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 106\n",
            "INFO:convert:[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 106\n",
            "INFO:convert:[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 106\n",
            "INFO:convert:[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 106\n",
            "INFO:convert:[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 106\n",
            "INFO:convert:[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 106\n",
            "INFO:convert:[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 106\n",
            "INFO:convert:[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 107\n",
            "INFO:convert:[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 107\n",
            "INFO:convert:[196/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 108\n",
            "INFO:convert:[197/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 109\n",
            "INFO:convert:[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 111\n",
            "INFO:convert:[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 111\n",
            "INFO:convert:[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 111\n",
            "INFO:convert:[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 111\n",
            "INFO:convert:[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 112\n",
            "INFO:convert:[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 112\n",
            "INFO:convert:[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 112\n",
            "INFO:convert:[205/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 112\n",
            "INFO:convert:[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 112\n",
            "INFO:convert:[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 113\n",
            "INFO:convert:[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 114\n",
            "INFO:convert:[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 114\n",
            "INFO:convert:[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 114\n",
            "INFO:convert:[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 114\n",
            "INFO:convert:[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 115\n",
            "INFO:convert:[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 115\n",
            "INFO:convert:[214/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 116\n",
            "INFO:convert:[215/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 116\n",
            "INFO:convert:[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 116\n",
            "INFO:convert:[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 116\n",
            "INFO:convert:[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 116\n",
            "INFO:convert:[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 116\n",
            "INFO:convert:[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 117\n",
            "INFO:convert:[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 117\n",
            "INFO:convert:[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 118\n",
            "INFO:convert:[223/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 119\n",
            "INFO:convert:[224/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 119\n",
            "INFO:convert:[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 120\n",
            "INFO:convert:[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 120\n",
            "INFO:convert:[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 120\n",
            "INFO:convert:[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 121\n",
            "INFO:convert:[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 121\n",
            "INFO:convert:[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 121\n",
            "INFO:convert:[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 121\n",
            "INFO:convert:[232/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 123\n",
            "INFO:convert:[233/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 124\n",
            "INFO:convert:[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 125\n",
            "INFO:convert:[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 125\n",
            "INFO:convert:[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 125\n",
            "INFO:convert:[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 125\n",
            "INFO:convert:[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 125\n",
            "INFO:convert:[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 125\n",
            "INFO:convert:[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 125\n",
            "INFO:convert:[241/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 130\n",
            "INFO:convert:[242/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 130\n",
            "INFO:convert:[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 131\n",
            "INFO:convert:[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 131\n",
            "INFO:convert:[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 131\n",
            "INFO:convert:[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 131\n",
            "INFO:convert:[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 131\n",
            "INFO:convert:[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 135\n",
            "INFO:convert:[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 135\n",
            "INFO:convert:[250/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 135\n",
            "INFO:convert:[251/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 136\n",
            "INFO:convert:[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 140\n",
            "INFO:convert:[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 140\n",
            "INFO:convert:[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 140\n",
            "INFO:convert:[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 140\n",
            "INFO:convert:[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 140\n",
            "INFO:convert:[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 140\n",
            "INFO:convert:[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 141\n",
            "INFO:convert:[259/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 143\n",
            "INFO:convert:[260/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 144\n",
            "INFO:convert:[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 145\n",
            "INFO:convert:[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 145\n",
            "INFO:convert:[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 145\n",
            "INFO:convert:[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 145\n",
            "INFO:convert:[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 145\n",
            "INFO:convert:[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 145\n",
            "INFO:convert:[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 145\n",
            "INFO:convert:[268/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 146\n",
            "INFO:convert:[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 147\n",
            "INFO:convert:[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 147\n",
            "INFO:convert:[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 147\n",
            "INFO:convert:[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 147\n",
            "INFO:convert:[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 147\n",
            "INFO:convert:[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 147\n",
            "INFO:convert:[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 148\n",
            "INFO:convert:[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 150\n",
            "INFO:convert:[277/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 151\n",
            "INFO:convert:[278/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 151\n",
            "INFO:convert:[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 152\n",
            "INFO:convert:[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 152\n",
            "INFO:convert:[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 152\n",
            "INFO:convert:[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 152\n",
            "INFO:convert:[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 152\n",
            "INFO:convert:[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 152\n",
            "INFO:convert:[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 153\n",
            "INFO:convert:[286/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 155\n",
            "INFO:convert:[287/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 156\n",
            "INFO:convert:[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 156\n",
            "INFO:convert:[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 156\n",
            "INFO:convert:[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 156\n",
            "INFO:convert:[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+ 156\n",
            "INFO:convert:Wrote models/llama-2-7b-light-f16.gguf\n"
          ]
        }
      ],
      "source": [
        "# build f16\n",
        "! mkdir models\n",
        "! python3 llama.cpp/convert.py --outtype f16 --outfile models/llama-2-7b-light-f16.gguf llama-2-7b-light/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGTH2f5CR3kb",
        "outputId": "79f0f0ca-3a3b-479c-f59c-dbd7f55a961a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n",
            "main: build = 2878 (1265c670)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '../models/llama-2-7b-light-f16.gguf' to '../models/llama-2-7b-light-Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/llama-2-7b-light-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q4_K .. size =   250.00 MiB ->    70.31 MiB\n",
            "[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[   7/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[   8/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  16/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  17/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  25/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  26/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  34/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  35/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  43/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  44/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  52/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  53/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  61/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  62/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  70/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  71/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  79/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  80/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  88/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  89/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  97/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  98/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 106/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 107/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 115/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 116/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 124/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 125/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 133/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 134/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 142/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 143/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 151/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 152/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 160/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 161/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 169/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 170/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 178/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 179/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 187/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 188/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 196/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 197/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
            "[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 205/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 206/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
            "[ 214/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. "
          ]
        }
      ],
      "source": [
        "# quantize\n",
        "%cd /content/llama.cpp\n",
        "#! make\n",
        "! ./quantize ../models/llama-2-7b-light-f16.gguf ../models/llama-2-7b-light-Q4_K_M.gguf Q4_K_M\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4gEDeO9R3kc"
      },
      "outputs": [],
      "source": [
        "# upload to huggingface\n",
        "%cd models\n",
        "! huggingface-cli upload LightXXXXX/llama-2-7b-light-gguf . ."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}